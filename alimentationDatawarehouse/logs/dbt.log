[0m10:24:25.269303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7BC82B080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7BC82AFC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7BC82B110>]}


============================== 10:24:25.279284 | 932b1082-9fe5-4d81-a54d-9a420f831fae ==============================
[0m10:24:25.279284 [info ] [MainThread]: Running with dbt=1.8.9
[0m10:24:25.287331 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:24:25.362437 [info ] [MainThread]: dbt version: 1.8.9
[0m10:24:25.362437 [info ] [MainThread]: python version: 3.12.7
[0m10:24:25.362437 [info ] [MainThread]: python path: C:\Users\Moi\AppData\Local\Programs\Python\Python312\python.exe
[0m10:24:25.362437 [info ] [MainThread]: os info: Windows-10-10.0.18362-SP0
[0m10:24:25.382736 [info ] [MainThread]: Using profiles dir at C:\Users\Moi\.dbt
[0m10:24:25.384424 [info ] [MainThread]: Using profiles.yml file at C:\Users\Moi\.dbt\profiles.yml
[0m10:24:25.384424 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Moi\Documents\M2\BI\dbt\dbtProjetBi\alimentationDatawarehouse\dbt_project.yml
[0m10:24:26.105174 [info ] [MainThread]: Configuration:
[0m10:24:26.116547 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m10:24:26.116547 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:24:26.123582 [info ] [MainThread]: Required dependencies:
[0m10:24:26.125617 [debug] [MainThread]: Executing "git --help"
[0m10:24:26.330904 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:24:26.330904 [debug] [MainThread]: STDERR: "b''"
[0m10:24:26.330904 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:24:26.351274 [info ] [MainThread]: Connection test skipped since no profile was found
[0m10:24:26.353308 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:24:26.384185 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Could not find profile named 'alimentationDatawarehouse'


[0m10:24:26.422098 [debug] [MainThread]: Command `dbt debug` failed at 10:24:26.422098 after 1.54 seconds
[0m10:24:26.434346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7BC9DB980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7BC6FD640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7BC0AF860>]}
[0m10:24:26.434346 [debug] [MainThread]: Flushing usage events
[0m10:25:37.463269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B715A67740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71582E3C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7118E1B20>]}


============================== 10:25:37.471513 | c6d98539-81d7-4dd4-bc3c-a6e91d7aad2a ==============================
[0m10:25:37.471513 [info ] [MainThread]: Running with dbt=1.8.9
[0m10:25:37.473668 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:25:37.518398 [info ] [MainThread]: dbt version: 1.8.9
[0m10:25:37.518398 [info ] [MainThread]: python version: 3.12.7
[0m10:25:37.518398 [info ] [MainThread]: python path: C:\Users\Moi\AppData\Local\Programs\Python\Python312\python.exe
[0m10:25:37.518398 [info ] [MainThread]: os info: Windows-10-10.0.18362-SP0
[0m10:25:46.007321 [info ] [MainThread]: Using profiles dir at C:\Users\Moi\.dbt
[0m10:25:46.015380 [info ] [MainThread]: Using profiles.yml file at C:\Users\Moi\.dbt\profiles.yml
[0m10:25:46.015566 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Moi\Documents\M2\BI\dbt\dbtProjetBi\alimentationDatawarehouse\dbt_project.yml
[0m10:25:46.017599 [info ] [MainThread]: adapter type: bigquery
[0m10:25:46.017599 [info ] [MainThread]: adapter version: 1.8.3
[0m10:25:46.194479 [info ] [MainThread]: Configuration:
[0m10:25:46.202621 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:25:46.202621 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:25:46.204689 [info ] [MainThread]: Required dependencies:
[0m10:25:46.204689 [debug] [MainThread]: Executing "git --help"
[0m10:25:46.257809 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:25:46.257809 [debug] [MainThread]: STDERR: "b''"
[0m10:25:46.257809 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:25:46.257809 [info ] [MainThread]: Connection:
[0m10:25:46.257809 [info ] [MainThread]:   method: service-account
[0m10:25:46.257809 [info ] [MainThread]:   database: projet-bi-isen
[0m10:25:46.266016 [info ] [MainThread]:   execution_project: projet-bi-isen
[0m10:25:46.266016 [info ] [MainThread]:   schema: dataWarehouse
[0m10:25:46.266016 [info ] [MainThread]:   location: EU
[0m10:25:46.266016 [info ] [MainThread]:   priority: interactive
[0m10:25:46.274020 [info ] [MainThread]:   maximum_bytes_billed: None
[0m10:25:46.274020 [info ] [MainThread]:   impersonate_service_account: None
[0m10:25:46.274020 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m10:25:46.274020 [info ] [MainThread]:   job_retries: 1
[0m10:25:46.274020 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m10:25:46.274020 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m10:25:46.282147 [info ] [MainThread]:   timeout_seconds: 300
[0m10:25:46.282369 [info ] [MainThread]:   client_id: None
[0m10:25:46.282369 [info ] [MainThread]:   token_uri: None
[0m10:25:46.282369 [info ] [MainThread]:   dataproc_region: None
[0m10:25:46.282369 [info ] [MainThread]:   dataproc_cluster_name: None
[0m10:25:46.290370 [info ] [MainThread]:   gcs_bucket: None
[0m10:25:46.290370 [info ] [MainThread]:   dataproc_batch: None
[0m10:25:46.290370 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m10:25:46.952500 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m10:25:46.952500 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:25:46.972879 [debug] [MainThread]: On debug: select 1 as id
[0m10:25:47.565250 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:5952699e-cc2d-4458-a3ec-34ac3731d081&page=queryresults
[0m10:25:47.864997 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m10:25:47.864997 [info ] [MainThread]: [32mAll checks passed![0m
[0m10:25:47.864997 [debug] [MainThread]: Command `dbt debug` succeeded at 10:25:47.864997 after 10.58 seconds
[0m10:25:47.873266 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:25:47.873266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7121593A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B715299F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B715B50380>]}
[0m10:25:47.873266 [debug] [MainThread]: Flushing usage events
[0m01:16:13.258892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016907441FA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001690741FB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169050A0B90>]}


============================== 01:16:13.267062 | 983d1eec-b965-4a56-8d71-43c233f28b2b ==============================
[0m01:16:13.267062 [info ] [MainThread]: Running with dbt=1.8.9
[0m01:16:13.275065 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select f_CommandeFournisseur', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:16:23.526012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001691A235D00>]}
[0m01:16:23.659135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001691887C1A0>]}
[0m01:16:23.659135 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m01:16:24.589928 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m01:16:24.589928 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m01:16:24.598130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001691A29F020>]}
[0m01:16:29.846675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001691A309430>]}
[0m01:16:30.109503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001691A79D370>]}
[0m01:16:30.109503 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m01:16:30.117692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001691A7BD9D0>]}
[0m01:16:30.117692 [info ] [MainThread]: 
[0m01:16:30.117692 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m01:16:30.125693 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m01:16:30.125693 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:16:30.562440 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m01:16:30.562440 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:16:30.835851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001691A7BDE50>]}
[0m01:16:30.835851 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:16:30.835851 [info ] [MainThread]: 
[0m01:16:30.843854 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m01:16:30.852079 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dataWarehouse.f_CommandeFournisseur ......... [RUN]
[0m01:16:30.854092 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.f_CommandeFournisseur'
[0m01:16:30.854092 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m01:16:30.878696 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m01:16:30.886702 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m01:16:30.987087 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:16:31.343663 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m01:16:31.343663 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m01:16:31.680428 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:2bb911b6-5d36-4a86-9ed7-3577345bf1b7&page=queryresults
[0m01:16:34.158938 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983d1eec-b965-4a56-8d71-43c233f28b2b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169074F3590>]}
[0m01:16:34.158938 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .... [[32mMERGE (10.0 rows, 10.2 KiB processed)[0m in 3.31s]
[0m01:16:34.158938 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m01:16:34.167179 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:16:34.167179 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m01:16:34.167179 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m01:16:34.169683 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_CommandeFournisseur' was properly closed.
[0m01:16:34.169683 [info ] [MainThread]: 
[0m01:16:34.169683 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 4.05 seconds (4.05s).
[0m01:16:34.169683 [debug] [MainThread]: Command end result
[0m01:16:34.259211 [info ] [MainThread]: 
[0m01:16:34.259211 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:16:34.259211 [info ] [MainThread]: 
[0m01:16:34.259211 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:16:34.267211 [debug] [MainThread]: Command `dbt run` succeeded at 01:16:34.267211 after 21.27 seconds
[0m01:16:34.267211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001690741FB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169180188F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016907918530>]}
[0m01:16:34.267211 [debug] [MainThread]: Flushing usage events
[0m10:22:06.079730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6EA76420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6F94C3E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6C365C10>]}


============================== 10:22:06.086861 | 5bad6a71-924a-4cf6-b2f5-43237bba9f5a ==============================
[0m10:22:06.086861 [info ] [MainThread]: Running with dbt=1.8.9
[0m10:22:06.086861 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select d_StatutCommande', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:22:11.944787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE72411430>]}
[0m10:22:12.004568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE048620C0>]}
[0m10:22:12.005566 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m10:22:12.429861 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m10:22:12.527361 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m10:22:12.528359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE71B2A210>]}
[0m10:22:14.949742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE04DB9C10>]}
[0m10:22:15.074632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE04E75DC0>]}
[0m10:22:15.075640 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m10:22:15.075640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE04DBC3E0>]}
[0m10:22:15.078616 [info ] [MainThread]: 
[0m10:22:15.078616 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:22:15.079594 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m10:22:15.080746 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:15.522609 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m10:22:15.522609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:15.815552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE04E6A990>]}
[0m10:22:15.816519 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:22:15.817516 [info ] [MainThread]: 
[0m10:22:15.822799 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m10:22:15.823801 [info ] [Thread-1 (]: 1 of 1 START sql view model dataWarehouse.d_StatutCommande ..................... [RUN]
[0m10:22:15.824784 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m10:22:15.824784 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m10:22:15.832989 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m10:22:15.834985 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_StatutCommande
[0m10:22:15.941943 [debug] [Thread-1 (]: Compilation Error in model d_StatutCommande (models\dim\d_StatutCommande.sql)
  Trying to create view `projet-bi-isen`.`dataWarehouse`.`d_StatutCommande`, but it currently exists as a table. Either drop `projet-bi-isen`.`dataWarehouse`.`d_StatutCommande` manually, or run dbt with `--full-refresh` and dbt will drop it for you.
  
  > in macro bigquery__handle_existing_table (macros\materializations\view.sql)
  > called by macro handle_existing_table (macros\relations\view\replace.sql)
  > called by macro bigquery__create_or_replace_view (macros\relations\view\replace.sql)
  > called by macro materialization_view_bigquery (macros\materializations\view.sql)
  > called by model d_StatutCommande (models\dim\d_StatutCommande.sql)
[0m10:22:15.943937 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5bad6a71-924a-4cf6-b2f5-43237bba9f5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE71B2B500>]}
[0m10:22:15.943937 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model dataWarehouse.d_StatutCommande ............ [[31mERROR[0m in 0.12s]
[0m10:22:15.945931 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m10:22:15.947926 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:22:15.948923 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m10:22:15.948923 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m10:22:15.948923 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.d_StatutCommande' was properly closed.
[0m10:22:15.949960 [info ] [MainThread]: 
[0m10:22:15.950096 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.87 seconds (0.87s).
[0m10:22:15.951096 [debug] [MainThread]: Command end result
[0m10:22:15.988561 [info ] [MainThread]: 
[0m10:22:15.991059 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m10:22:15.991059 [info ] [MainThread]: 
[0m10:22:15.992092 [error] [MainThread]:   Compilation Error in model d_StatutCommande (models\dim\d_StatutCommande.sql)
  Trying to create view `projet-bi-isen`.`dataWarehouse`.`d_StatutCommande`, but it currently exists as a table. Either drop `projet-bi-isen`.`dataWarehouse`.`d_StatutCommande` manually, or run dbt with `--full-refresh` and dbt will drop it for you.
  
  > in macro bigquery__handle_existing_table (macros\materializations\view.sql)
  > called by macro handle_existing_table (macros\relations\view\replace.sql)
  > called by macro bigquery__create_or_replace_view (macros\relations\view\replace.sql)
  > called by macro materialization_view_bigquery (macros\materializations\view.sql)
  > called by model d_StatutCommande (models\dim\d_StatutCommande.sql)
[0m10:22:15.994052 [info ] [MainThread]: 
[0m10:22:15.995080 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m10:22:15.996047 [debug] [MainThread]: Command `dbt run` failed at 10:22:15.996047 after 10.07 seconds
[0m10:22:15.997080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6EA76420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE71B2B500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE71FC24E0>]}
[0m10:22:15.997080 [debug] [MainThread]: Flushing usage events
[0m10:23:49.043186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D8BA5C470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D8E3D5040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D8E3D62D0>]}


============================== 10:23:49.047176 | 37a8dac9-aed1-436e-bec0-69a76259ea0e ==============================
[0m10:23:49.047176 [info ] [MainThread]: Running with dbt=1.8.9
[0m10:23:49.048174 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --select d_StatutCommande', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:23:50.182529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '37a8dac9-aed1-436e-bec0-69a76259ea0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D8E5203B0>]}
[0m10:23:50.240262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '37a8dac9-aed1-436e-bec0-69a76259ea0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D9E963DA0>]}
[0m10:23:50.241264 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m10:23:50.506619 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m10:23:50.664264 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:23:50.665270 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_StatutCommande.sql
[0m10:23:50.903034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '37a8dac9-aed1-436e-bec0-69a76259ea0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DA0B570E0>]}
[0m10:23:51.000686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '37a8dac9-aed1-436e-bec0-69a76259ea0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DA0AF6E70>]}
[0m10:23:51.000686 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m10:23:51.001683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37a8dac9-aed1-436e-bec0-69a76259ea0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DA0D5D0A0>]}
[0m10:23:51.003677 [info ] [MainThread]: 
[0m10:23:51.004675 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:23:51.009662 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m10:23:51.009662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:51.268765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37a8dac9-aed1-436e-bec0-69a76259ea0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DA0AE1880>]}
[0m10:23:51.268765 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:23:51.270074 [info ] [MainThread]: 
[0m10:23:51.274671 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m10:23:51.274671 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m10:23:51.274671 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m10:23:51.283479 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m10:23:51.285450 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m10:23:51.286454 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:23:51.287460 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m10:23:51.287460 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.d_StatutCommande' was properly closed.
[0m10:23:51.287460 [info ] [MainThread]: 
[0m10:23:51.288440 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m10:23:51.289435 [debug] [MainThread]: Command end result
[0m10:23:51.385048 [debug] [MainThread]: Command `dbt run` succeeded at 10:23:51.385048 after 2.44 seconds
[0m10:23:51.386076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D8E167D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D9EB7D880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021DA0F6D760>]}
[0m10:23:51.386076 [debug] [MainThread]: Flushing usage events
[0m11:13:35.875668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E460D40950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E463166BA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4639F8CE0>]}


============================== 11:13:35.884225 | 59fcfcfd-b2f1-45a6-89eb-397b81342d0a ==============================
[0m11:13:35.884225 [info ] [MainThread]: Running with dbt=1.8.9
[0m11:13:35.884905 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:13:38.322539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4601792E0>]}
[0m11:13:38.377253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E462C74170>]}
[0m11:13:38.378251 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m11:13:38.659089 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m11:13:38.838184 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:13:38.839181 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\ODS\f_commandeInternet.sql
[0m11:13:39.077804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4761BEED0>]}
[0m11:13:39.193007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E476131730>]}
[0m11:13:39.193007 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m11:13:39.194006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E474C68BC0>]}
[0m11:13:39.196000 [info ] [MainThread]: 
[0m11:13:39.196998 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:13:39.202102 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m11:13:39.203101 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:13:39.586727 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m11:13:39.587694 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:13:39.825538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4760C1430>]}
[0m11:13:39.825538 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:13:39.826538 [info ] [MainThread]: 
[0m11:13:39.836404 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:13:39.837397 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m11:13:39.837397 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m11:13:39.845493 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m11:13:39.847455 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:13:39.847455 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:13:39.848452 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m11:13:39.848452 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m11:13:39.850645 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m11:13:39.851643 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:13:39.852641 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:13:39.852641 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m11:13:39.853638 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m11:13:39.855661 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m11:13:39.857629 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:13:39.858626 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m11:13:39.858626 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m11:13:39.860043 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m11:13:39.861077 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m11:13:39.863036 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m11:13:39.863036 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:13:39.864033 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m11:13:39.865030 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m11:13:39.865030 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:13:39.868021 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:13:39.869020 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:13:39.885409 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:40.197457 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:13:40.198460 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN "2024-11-26" AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m11:13:40.615933 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4c762f5b-bd22-4bf4-8ff3-19dda01c1441&page=queryresults
[0m11:13:42.973124 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E463283950>]}
[0m11:13:42.974123 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (4.0 rows, 433.0 Bytes processed)[0m in 3.11s]
[0m11:13:42.975078 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:13:42.976076 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:13:42.976076 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m11:13:42.976829 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m11:13:42.976829 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:13:42.979954 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:13:42.981951 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:13:42.984942 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:43.226369 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:13:43.228325 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-11-26' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:13:43.657931 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:eade5279-450e-4fd2-be79-a24f31646bc3&page=queryresults
[0m11:13:45.987318 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E47654B110>]}
[0m11:13:45.987318 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (7.0 rows, 958.0 Bytes processed)[0m in 3.01s]
[0m11:13:45.989545 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:13:45.989545 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m11:13:45.989545 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m11:13:45.990580 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m11:13:45.991585 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m11:13:45.994534 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m11:13:45.996527 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m11:13:46.000956 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:46.230215 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m11:13:46.231436 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-11-26' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:13:46.650150 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4b80fa1e-d1c2-4dcf-aa69-c718509f1f7f&page=queryresults
[0m11:13:48.953831 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4760C0080>]}
[0m11:13:48.954831 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (9.0 rows, 824.0 Bytes processed)[0m in 2.96s]
[0m11:13:48.955590 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m11:13:48.956628 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m11:13:48.956628 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m11:13:48.957597 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m11:13:48.957597 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m11:13:48.967945 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m11:13:48.969172 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m11:13:49.003906 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:49.292786 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m11:13:49.294744 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        DISTINCT transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m11:13:49.760073 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:7c42465a-7409-4e30-a797-c593ae22d8cd&page=queryresults
[0m11:13:51.780247 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E476756EA0>]}
[0m11:13:51.781286 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (2.0 rows, 182.0 Bytes processed)[0m in 2.82s]
[0m11:13:51.782243 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m11:13:51.783241 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m11:13:51.783241 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m11:13:51.784238 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m11:13:51.785270 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m11:13:51.789452 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:13:51.790453 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m11:13:51.795450 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:52.125488 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:13:52.127499 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m11:13:52.593035 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4b9daa7c-95eb-475f-8d95-b85136377aa1&page=queryresults
[0m11:13:54.829522 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E476712D80>]}
[0m11:13:54.830556 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (2.0 rows, 20.0 Bytes processed)[0m in 3.05s]
[0m11:13:54.831518 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m11:13:54.832516 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m11:13:54.832516 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m11:13:54.833543 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m11:13:54.833543 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m11:13:54.837534 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:13:54.839537 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m11:13:54.844547 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:55.186272 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:13:55.188226 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m11:13:55.650104 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:3af7def8-045d-4c86-bc9e-3e858c3e2bcf&page=queryresults
[0m11:13:57.638128 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E47673A240>]}
[0m11:13:57.639124 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (2.0 rows, 357.0 Bytes processed)[0m in 2.80s]
[0m11:13:57.640278 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m11:13:57.640278 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:13:57.641312 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m11:13:57.642273 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m11:13:57.642273 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:13:57.646273 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:13:57.647261 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:13:57.650758 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:57.899763 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:13:57.900542 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m11:13:58.338481 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:3a23211b-2549-4a6b-96c1-fe108b22b4ad&page=queryresults
[0m11:14:00.591966 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4765556A0>]}
[0m11:14:00.591966 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (2.0 rows, 340.0 Bytes processed)[0m in 2.95s]
[0m11:14:00.593925 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:14:00.594919 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m11:14:00.594919 [info ] [Thread-1 (]: 8 of 13 START sql incremental model dataWarehouse.d_Materiaux .................. [RUN]
[0m11:14:00.595954 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m11:14:00.595954 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m11:14:00.599948 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:14:00.602939 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m11:14:00.606927 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:14:00.830468 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:14:00.832463 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        materiaux_nom AS MateriauxNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS MateriauxID,
    MateriauxNom,
    null AS valeurEstimee
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` AS target
        WHERE target.MateriauxNom = base.MateriauxNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)
    values
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)


    
[0m11:14:01.334346 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:9b00cbaa-7d9a-4c13-84dd-b4dc2e322e15&page=queryresults
[0m11:14:03.498527 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4767B7680>]}
[0m11:14:03.499525 [info ] [Thread-1 (]: 8 of 13 OK created sql incremental model dataWarehouse.d_Materiaux ............. [[32mMERGE (7.0 rows, 188.0 Bytes processed)[0m in 2.90s]
[0m11:14:03.500583 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m11:14:03.500583 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m11:14:03.501581 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m11:14:03.502577 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m11:14:03.502577 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m11:14:03.507564 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m11:14:03.509559 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m11:14:03.512805 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:14:03.750110 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m11:14:03.751322 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`)


    
[0m11:14:04.215982 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:63642fe6-0244-4351-aac4-768f34372bfa&page=queryresults
[0m11:14:06.741959 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E475FB5B20>]}
[0m11:14:06.742957 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (8.0 rows, 232.0 Bytes processed)[0m in 3.24s]
[0m11:14:06.743929 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m11:14:06.743929 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m11:14:06.744922 [info ] [Thread-1 (]: 10 of 13 START sql incremental model dataWarehouse.d_Produit ................... [RUN]
[0m11:14:06.745932 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m11:14:06.745932 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m11:14:06.750063 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m11:14:06.753057 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m11:14:06.756078 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:14:06.999054 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m11:14:07.000181 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Produit` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        distinct produit_nom AS ProduitNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ProduitID,
    ProduitNom,
    null AS CoutdeRevient
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` AS target
        WHERE target.ProduitNom = base.ProduitNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)
    values
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)


    
[0m11:14:07.512306 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:73d77536-9384-4c8a-a822-9d4e66be7688&page=queryresults
[0m11:14:09.767094 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4765B8F50>]}
[0m11:14:09.768096 [info ] [Thread-1 (]: 10 of 13 OK created sql incremental model dataWarehouse.d_Produit .............. [[32mMERGE (3.0 rows, 144.0 Bytes processed)[0m in 3.02s]
[0m11:14:09.769250 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m11:14:09.769250 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:14:09.770283 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m11:14:09.771244 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m11:14:09.771244 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:14:09.775233 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:14:09.776231 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:14:09.781721 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:14:10.028809 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:14:10.030085 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m11:14:10.506765 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:899ee57e-1821-4c68-84cb-d6b4eb01e3b0&page=queryresults
[0m11:14:12.706008 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E47675FFB0>]}
[0m11:14:12.706008 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (14.0 rows, 1.6 KiB processed)[0m in 2.93s]
[0m11:14:12.707003 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:14:12.708003 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:14:12.709002 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m11:14:12.709002 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m11:14:12.710235 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m11:14:12.728394 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:14:12.729613 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m11:14:12.736595 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:14:12.984587 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:14:12.985549 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m11:14:13.448659 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:55dd32b9-c691-456d-9275-e56210a2849a&page=queryresults
[0m11:14:15.700409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4767BBEF0>]}
[0m11:14:15.701375 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (4.0 rows, 1.4 KiB processed)[0m in 2.99s]
[0m11:14:15.703370 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:14:15.703370 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m11:14:15.704367 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m11:14:15.705365 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m11:14:15.705365 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m11:14:15.709354 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m11:14:15.711351 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m11:14:15.714367 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:14:15.932042 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m11:14:15.934027 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m11:14:16.391988 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:bb92141a-e1b3-4e4e-be62-3bc1743cbe80&page=queryresults
[0m11:14:19.205356 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59fcfcfd-b2f1-45a6-89eb-397b81342d0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E474A2DA00>]}
[0m11:14:19.206355 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (18.0 rows, 1.8 KiB processed)[0m in 3.50s]
[0m11:14:19.207316 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m11:14:19.209583 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:14:19.209583 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m11:14:19.209583 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m11:14:19.209583 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m11:14:19.211672 [info ] [MainThread]: 
[0m11:14:19.212361 [info ] [MainThread]: Finished running 3 table models, 10 incremental models in 0 hours 0 minutes and 40.01 seconds (40.01s).
[0m11:14:19.217388 [debug] [MainThread]: Command end result
[0m11:14:19.254457 [info ] [MainThread]: 
[0m11:14:19.255421 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:14:19.256417 [info ] [MainThread]: 
[0m11:14:19.256417 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m11:14:19.257416 [debug] [MainThread]: Command `dbt run` succeeded at 11:14:19.257416 after 43.56 seconds
[0m11:14:19.258444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4637338C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4637301D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E463732420>]}
[0m11:14:19.258444 [debug] [MainThread]: Flushing usage events
[0m11:21:43.794137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024423631C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024423633470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024423630AD0>]}


============================== 11:21:43.798059 | 65089100-a7a8-48cd-a654-b87bffbd4292 ==============================
[0m11:21:43.798059 [info ] [MainThread]: Running with dbt=1.8.9
[0m11:21:43.799031 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select d_Entrepot', 'send_anonymous_usage_stats': 'True'}
[0m11:21:44.967808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '65089100-a7a8-48cd-a654-b87bffbd4292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002442198E6F0>]}
[0m11:21:45.024554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '65089100-a7a8-48cd-a654-b87bffbd4292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024423F4B650>]}
[0m11:21:45.025557 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m11:21:45.295616 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m11:21:45.459916 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:21:45.459916 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_Entrepot.sql
[0m11:21:45.705512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65089100-a7a8-48cd-a654-b87bffbd4292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244364F6990>]}
[0m11:21:45.805626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65089100-a7a8-48cd-a654-b87bffbd4292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024436A75970>]}
[0m11:21:45.805626 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m11:21:45.806634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65089100-a7a8-48cd-a654-b87bffbd4292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024436A6E720>]}
[0m11:21:45.808648 [info ] [MainThread]: 
[0m11:21:45.808648 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:21:45.809904 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m11:21:45.810932 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:46.178846 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m11:21:46.180059 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:46.413554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65089100-a7a8-48cd-a654-b87bffbd4292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002443646BC20>]}
[0m11:21:46.414516 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:21:46.415523 [info ] [MainThread]: 
[0m11:21:46.419511 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m11:21:46.419511 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dataWarehouse.d_Entrepot .................... [RUN]
[0m11:21:46.420850 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_Entrepot'
[0m11:21:46.421880 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m11:21:46.433133 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:21:46.434129 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m11:21:46.474062 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:21:46.786965 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:21:46.787962 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m11:21:47.501775 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:94377655-77f8-4906-8543-441c97858595&page=queryresults
[0m11:21:49.777307 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65089100-a7a8-48cd-a654-b87bffbd4292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244369AF590>]}
[0m11:21:49.778306 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dataWarehouse.d_Entrepot ............... [[32mMERGE (1.0 rows, 357.0 Bytes processed)[0m in 3.36s]
[0m11:21:49.779266 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m11:21:49.781543 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:21:49.781543 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m11:21:49.781543 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m11:21:49.782573 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.d_Entrepot' was properly closed.
[0m11:21:49.782573 [info ] [MainThread]: 
[0m11:21:49.783529 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.97 seconds (3.97s).
[0m11:21:49.784527 [debug] [MainThread]: Command end result
[0m11:21:49.820365 [info ] [MainThread]: 
[0m11:21:49.820365 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:21:49.821677 [info ] [MainThread]: 
[0m11:21:49.822268 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:21:49.822905 [debug] [MainThread]: Command `dbt run` succeeded at 11:21:49.822905 after 6.16 seconds
[0m11:21:49.823904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244231CF680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024423A5A060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024436905FA0>]}
[0m11:21:49.823904 [debug] [MainThread]: Flushing usage events
[0m11:24:57.562066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C5D5F07D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C600F23F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C600F1010>]}


============================== 11:24:57.565888 | 2d613b33-c339-45aa-bec3-da5cddf12c89 ==============================
[0m11:24:57.565888 [info ] [MainThread]: Running with dbt=1.8.9
[0m11:24:57.566887 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select d_Client', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:24:58.743608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2d613b33-c339-45aa-bec3-da5cddf12c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C5DB5ED20>]}
[0m11:24:58.801876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2d613b33-c339-45aa-bec3-da5cddf12c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C71382AB0>]}
[0m11:24:58.802900 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m11:24:59.075529 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m11:24:59.233333 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:24:59.234324 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_Client.sql
[0m11:24:59.483806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2d613b33-c339-45aa-bec3-da5cddf12c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C72C30E90>]}
[0m11:24:59.587450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2d613b33-c339-45aa-bec3-da5cddf12c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C72ADDFD0>]}
[0m11:24:59.588448 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m11:24:59.589489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2d613b33-c339-45aa-bec3-da5cddf12c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C727210D0>]}
[0m11:24:59.590733 [info ] [MainThread]: 
[0m11:24:59.591690 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:24:59.592735 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m11:24:59.592735 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:59.953438 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m11:24:59.954436 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:00.235580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2d613b33-c339-45aa-bec3-da5cddf12c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C72B6FF20>]}
[0m11:25:00.235580 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:25:00.236811 [info ] [MainThread]: 
[0m11:25:00.241103 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m11:25:00.241103 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dataWarehouse.d_Client ...................... [RUN]
[0m11:25:00.242100 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_Client'
[0m11:25:00.243101 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m11:25:00.255352 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m11:25:00.256344 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m11:25:00.294005 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:25:00.595698 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m11:25:00.596695 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_adresse 
            ORDER BY transaction_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m11:25:00.725231 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:c04a134c-dbeb-45f9-8bbc-db2189c8dd4e&page=queryresults
[0m11:25:00.726256 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got identifier "ROW_NUMBER" at [20:9]; reason: invalidQuery, location: query, message: Syntax error: Expected ")" but got identifier "ROW_NUMBER" at [20:9]')
[0m11:25:01.589534 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:ed3d65b3-1d32-4dc1-8d2a-1d6caf69a44a&page=queryresults
[0m11:25:01.589534 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:ed3d65b3-1d32-4dc1-8d2a-1d6caf69a44a&page=queryresults
[0m11:25:01.701310 [debug] [Thread-1 (]: Database Error in model d_Client (models\dim\d_Client.sql)
  Syntax error: Expected ")" but got identifier "ROW_NUMBER" at [20:9]
  compiled code at target\run\alimentationDatawarehouse\models\dim\d_Client.sql
[0m11:25:01.702305 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d613b33-c339-45aa-bec3-da5cddf12c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C7119E000>]}
[0m11:25:01.703309 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dataWarehouse.d_Client ............. [[31mERROR[0m in 1.46s]
[0m11:25:01.704263 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m11:25:01.706258 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:25:01.706258 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m11:25:01.706258 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m11:25:01.707293 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.d_Client' was properly closed.
[0m11:25:01.707293 [info ] [MainThread]: 
[0m11:25:01.708253 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 2.12 seconds (2.12s).
[0m11:25:01.709250 [debug] [MainThread]: Command end result
[0m11:25:01.852227 [info ] [MainThread]: 
[0m11:25:01.852227 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:25:01.853223 [info ] [MainThread]: 
[0m11:25:01.854220 [error] [MainThread]:   Database Error in model d_Client (models\dim\d_Client.sql)
  Syntax error: Expected ")" but got identifier "ROW_NUMBER" at [20:9]
  compiled code at target\run\alimentationDatawarehouse\models\dim\d_Client.sql
[0m11:25:01.854220 [info ] [MainThread]: 
[0m11:25:01.855218 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:25:01.856215 [debug] [MainThread]: Command `dbt run` failed at 11:25:01.856215 after 4.42 seconds
[0m11:25:01.857212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C70FCEC00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C70FCF650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C70F8A600>]}
[0m11:25:01.857212 [debug] [MainThread]: Flushing usage events
[0m11:25:54.890117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3EA367500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3EA367020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3EA9B7B30>]}


============================== 11:25:54.894108 | c3132175-5dec-4db6-b378-4574dc17f542 ==============================
[0m11:25:54.894108 [info ] [MainThread]: Running with dbt=1.8.9
[0m11:25:54.895106 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --select d_Client', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:25:56.058496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3132175-5dec-4db6-b378-4574dc17f542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3E9DD8560>]}
[0m11:25:56.118241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3132175-5dec-4db6-b378-4574dc17f542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3EAE33650>]}
[0m11:25:56.118241 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m11:25:56.377468 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m11:25:56.537627 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:25:56.538624 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_Client.sql
[0m11:25:56.791290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c3132175-5dec-4db6-b378-4574dc17f542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3FD8C3050>]}
[0m11:25:56.895668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3132175-5dec-4db6-b378-4574dc17f542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3FD94D8B0>]}
[0m11:25:56.895668 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m11:25:56.896669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3132175-5dec-4db6-b378-4574dc17f542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3FD94B680>]}
[0m11:25:56.898688 [info ] [MainThread]: 
[0m11:25:56.899818 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:25:56.900743 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m11:25:56.900743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:57.254910 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m11:25:57.255907 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:57.480559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3132175-5dec-4db6-b378-4574dc17f542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3FD7FD3D0>]}
[0m11:25:57.481587 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:25:57.481587 [info ] [MainThread]: 
[0m11:25:57.485567 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m11:25:57.486593 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dataWarehouse.d_Client ...................... [RUN]
[0m11:25:57.487592 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_Client'
[0m11:25:57.487592 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m11:25:57.501010 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m11:25:57.501998 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m11:25:57.540237 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:25:57.865520 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m11:25:57.867477 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_adresse 
            ORDER BY transaction_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m11:25:58.328710 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:bdd6ce71-2020-4c9f-99b4-c113c245e5d1&page=queryresults
[0m11:26:00.326867 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3132175-5dec-4db6-b378-4574dc17f542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3FD88F230>]}
[0m11:26:00.327868 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dataWarehouse.d_Client ................. [[32mMERGE (0.0 rows, 255.0 Bytes processed)[0m in 2.84s]
[0m11:26:00.328848 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m11:26:00.331063 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:26:00.332061 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m11:26:00.332061 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m11:26:00.332061 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.d_Client' was properly closed.
[0m11:26:00.332061 [info ] [MainThread]: 
[0m11:26:00.333094 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.43 seconds (3.43s).
[0m11:26:00.335053 [debug] [MainThread]: Command end result
[0m11:26:00.372836 [info ] [MainThread]: 
[0m11:26:00.373796 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:26:00.374799 [info ] [MainThread]: 
[0m11:26:00.375791 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:26:00.376789 [debug] [MainThread]: Command `dbt run` succeeded at 11:26:00.376789 after 5.58 seconds
[0m11:26:00.377788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3FD8E6C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3EAB0FCE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3EAB2EC00>]}
[0m11:26:00.378783 [debug] [MainThread]: Flushing usage events
[0m11:35:11.395734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C597B2450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C58D8E060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C56D4C7A0>]}


============================== 11:35:11.399647 | 01f25c3f-d0fc-4f89-b49f-985de640b9d4 ==============================
[0m11:35:11.399647 [info ] [MainThread]: Running with dbt=1.8.9
[0m11:35:11.400741 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:35:12.570952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C07C230>]}
[0m11:35:12.628132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6BEB1C70>]}
[0m11:35:12.629366 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m11:35:12.897505 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m11:35:13.072894 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m11:35:13.072894 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_FournisseurDetails.sql
[0m11:35:13.073891 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_Client.sql
[0m11:35:13.073891 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_Materiaux.sql
[0m11:35:13.329572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C65DE50>]}
[0m11:35:13.428840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C40F7D0>]}
[0m11:35:13.428840 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m11:35:13.429989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C6A1BB0>]}
[0m11:35:13.432985 [info ] [MainThread]: 
[0m11:35:13.433979 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:35:13.439964 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m11:35:13.439964 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:35:13.768002 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m11:35:13.768002 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:35:14.005948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C6A2E70>]}
[0m11:35:14.006914 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:35:14.007911 [info ] [MainThread]: 
[0m11:35:14.011920 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:35:14.011920 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m11:35:14.012952 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m11:35:14.022204 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m11:35:14.024201 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:35:14.025205 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:35:14.026194 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m11:35:14.027192 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m11:35:14.029219 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m11:35:14.030016 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:35:14.031016 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:35:14.031016 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m11:35:14.031016 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m11:35:14.034046 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m11:35:14.035005 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:35:14.035005 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m11:35:14.036003 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m11:35:14.036003 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m11:35:14.102481 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m11:35:14.103477 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m11:35:14.104467 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:35:14.104467 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m11:35:14.105444 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m11:35:14.106471 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:35:14.108468 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:35:14.109769 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:35:14.126065 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:14.383837 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:35:14.384847 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN "2024-11-26" AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m11:35:14.863719 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:3d7261e8-4fe9-453a-adbf-2876923899b7&page=queryresults
[0m11:35:17.543951 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C6A2FC0>]}
[0m11:35:17.544951 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (4.0 rows, 433.0 Bytes processed)[0m in 3.44s]
[0m11:35:17.545911 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:35:17.545911 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:35:17.546943 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m11:35:17.547906 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m11:35:17.547906 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:35:17.551897 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:35:17.552895 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:35:17.557891 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:17.956987 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:35:17.957997 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-11-26' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:35:18.458968 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:84e9f457-c833-4455-bf97-f7b9ee0a962b&page=queryresults
[0m11:35:20.797554 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C592E34A0>]}
[0m11:35:20.797554 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (7.0 rows, 958.0 Bytes processed)[0m in 3.25s]
[0m11:35:20.799819 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:35:20.799819 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m11:35:20.800852 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m11:35:20.801814 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m11:35:20.801814 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m11:35:20.804852 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m11:35:20.805806 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m11:35:20.810303 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:21.107227 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m11:35:21.109184 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-11-26' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:35:21.532352 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:09311f2a-0ebe-4cdb-8ec4-2979b069b887&page=queryresults
[0m11:35:23.898834 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C798890>]}
[0m11:35:23.900030 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (9.0 rows, 824.0 Bytes processed)[0m in 3.10s]
[0m11:35:23.901043 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m11:35:23.901043 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m11:35:23.902044 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m11:35:23.902811 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m11:35:23.902811 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m11:35:23.909831 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m11:35:23.911018 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m11:35:23.948873 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:24.225295 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m11:35:24.227257 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_adresse 
            ORDER BY transaction_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m11:35:24.694332 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:edacdbae-e8f9-4770-8b9e-6066d737ad1c&page=queryresults
[0m11:35:26.612303 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C79A3C0>]}
[0m11:35:26.613311 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (2.0 rows, 182.0 Bytes processed)[0m in 2.71s]
[0m11:35:26.614261 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m11:35:26.614261 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m11:35:26.615298 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m11:35:26.616282 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m11:35:26.617293 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m11:35:26.620524 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:35:26.622522 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m11:35:26.624546 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:26.870548 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:35:26.871539 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m11:35:27.363302 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:ec747950-ec0b-4302-8f8f-84584a5515b1&page=queryresults
[0m11:35:29.572213 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C79BEC0>]}
[0m11:35:29.573211 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (2.0 rows, 20.0 Bytes processed)[0m in 2.96s]
[0m11:35:29.574169 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m11:35:29.574169 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m11:35:29.575166 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m11:35:29.576165 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m11:35:29.576165 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m11:35:29.580448 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:35:29.581446 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m11:35:29.586466 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:29.853868 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:35:29.854863 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m11:35:30.349091 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:65379011-7918-4d62-b6d9-6676f43fc7f4&page=queryresults
[0m11:35:32.288334 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C65ECF0>]}
[0m11:35:32.289645 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (1.0 rows, 357.0 Bytes processed)[0m in 2.71s]
[0m11:35:32.290642 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m11:35:32.291640 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:32.291640 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m11:35:32.292638 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m11:35:32.292638 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:32.296663 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:35:32.297628 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:32.301952 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:32.545917 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:35:32.546877 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
        ROW_NUMBER() OVER (
            PARTITION BY fournisseur_nom, fournisseur_contact, fournisseur_adresse 
            ORDER BY provisionnement_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base
WHERE rn = 1

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m11:35:32.863142 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:b63b476a-3c40-4281-a0fb-2c4f60fb4801&page=queryresults
[0m11:35:35.082091 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C161040>]}
[0m11:35:35.083085 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (2.0 rows, 340.0 Bytes processed)[0m in 2.79s]
[0m11:35:35.084048 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:35.085045 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:35.085045 [info ] [Thread-1 (]: 8 of 13 START sql incremental model dataWarehouse.d_Materiaux .................. [RUN]
[0m11:35:35.086191 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m11:35:35.087228 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:35.090186 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:35:35.092181 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:35.096205 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:35.302158 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:35:35.303120 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT materiaux_nom AS MateriauxNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS MateriauxID,
    MateriauxNom,
    null AS valeurEstimee
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` AS target
        WHERE target.MateriauxNom = base.MateriauxNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)
    values
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)


    
[0m11:35:35.733261 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:59132f03-44c9-4f67-b3ec-6125898795c9&page=queryresults
[0m11:35:37.642567 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C162BA0>]}
[0m11:35:37.643564 [info ] [Thread-1 (]: 8 of 13 OK created sql incremental model dataWarehouse.d_Materiaux ............. [[32mMERGE (7.0 rows, 90.0 Bytes processed)[0m in 2.56s]
[0m11:35:37.644562 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:37.645559 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m11:35:37.645559 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m11:35:37.646557 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m11:35:37.646557 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m11:35:37.650903 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m11:35:37.652865 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m11:35:37.656891 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:37.910096 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m11:35:37.910270 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`)


    
[0m11:35:38.374613 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:fdeb234f-a4f8-4049-bd9f-ffb2d0490835&page=queryresults
[0m11:35:40.602446 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C5B8920>]}
[0m11:35:40.603443 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (8.0 rows, 232.0 Bytes processed)[0m in 2.96s]
[0m11:35:40.604468 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m11:35:40.605438 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m11:35:40.605438 [info ] [Thread-1 (]: 10 of 13 START sql incremental model dataWarehouse.d_Produit ................... [RUN]
[0m11:35:40.606469 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m11:35:40.607508 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m11:35:40.610807 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m11:35:40.611771 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m11:35:40.616786 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:40.876420 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m11:35:40.877380 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Produit` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        distinct produit_nom AS ProduitNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ProduitID,
    ProduitNom,
    null AS CoutdeRevient
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` AS target
        WHERE target.ProduitNom = base.ProduitNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)
    values
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)


    
[0m11:35:41.329045 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:0f892c75-c95f-4659-9d94-3db4e2ebbbd1&page=queryresults
[0m11:35:43.546337 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C6029F0>]}
[0m11:35:43.547342 [info ] [Thread-1 (]: 10 of 13 OK created sql incremental model dataWarehouse.d_Produit .............. [[32mMERGE (3.0 rows, 144.0 Bytes processed)[0m in 2.94s]
[0m11:35:43.548310 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m11:35:43.549293 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:43.549293 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m11:35:43.550496 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m11:35:43.550496 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:43.554488 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:35:43.556481 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:43.560784 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:43.800759 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:35:43.801757 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m11:35:44.296496 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:fd5d5d5e-d560-4f40-ad7b-4403916c269b&page=queryresults
[0m11:35:46.548004 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C7FE0F0>]}
[0m11:35:46.549005 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (7.0 rows, 1.6 KiB processed)[0m in 3.00s]
[0m11:35:46.550217 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:46.550217 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:46.551249 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m11:35:46.552212 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m11:35:46.552212 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:46.571522 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:35:46.572553 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:46.575541 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:46.837971 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:35:46.840109 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m11:35:47.306658 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:c588f684-9529-40f7-9f9d-338db418f0b4&page=queryresults
[0m11:35:49.556568 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C5A1640>]}
[0m11:35:49.556568 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (4.0 rows, 1.4 KiB processed)[0m in 3.00s]
[0m11:35:49.558520 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:49.558520 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m11:35:49.559742 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m11:35:49.560739 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m11:35:49.560739 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m11:35:49.564767 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m11:35:49.565727 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m11:35:49.571184 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:49.781804 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m11:35:49.782800 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m11:35:50.240082 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:e0ce5c06-e12e-4ecf-92ac-6e508ab70c22&page=queryresults
[0m11:35:52.816891 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01f25c3f-d0fc-4f89-b49f-985de640b9d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C7A9B80>]}
[0m11:35:52.816891 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (9.0 rows, 1.8 KiB processed)[0m in 3.26s]
[0m11:35:52.818132 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m11:35:52.820435 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:35:52.820435 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m11:35:52.820435 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m11:35:52.820435 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m11:35:52.821469 [info ] [MainThread]: 
[0m11:35:52.822148 [info ] [MainThread]: Finished running 3 table models, 10 incremental models in 0 hours 0 minutes and 39.39 seconds (39.39s).
[0m11:35:52.826152 [debug] [MainThread]: Command end result
[0m11:35:52.863068 [info ] [MainThread]: 
[0m11:35:52.864029 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:35:52.864029 [info ] [MainThread]: 
[0m11:35:52.865061 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m11:35:52.866042 [debug] [MainThread]: Command `dbt run` succeeded at 11:35:52.866042 after 41.60 seconds
[0m11:35:52.867045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C59783FB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C78A7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C78AED0>]}
[0m11:35:52.867045 [debug] [MainThread]: Flushing usage events
[0m11:53:07.001872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290019BA240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290019BB140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290019BA690>]}


============================== 11:53:07.010547 | c3028463-b158-41c6-8abc-429261eeffe6 ==============================
[0m11:53:07.010547 [info ] [MainThread]: Running with dbt=1.8.9
[0m11:53:07.011526 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:53:10.227002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002900215BAA0>]}
[0m11:53:10.285500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029012E1F680>]}
[0m11:53:10.285500 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m11:53:10.569391 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m11:53:10.822866 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m11:53:10.823870 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\staging\stg_CommandeClient.sql
[0m11:53:10.823870 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\staging\stg_CommandeFournisseur.sql
[0m11:53:10.823870 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\staging\stg_Production.sql
[0m11:53:11.073110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014D270E0>]}
[0m11:53:11.191102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014A83800>]}
[0m11:53:11.192071 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m11:53:11.193068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014DD26C0>]}
[0m11:53:11.195101 [info ] [MainThread]: 
[0m11:53:11.196060 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:53:11.202388 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m11:53:11.203350 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:53:11.619129 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m11:53:11.619129 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:53:11.891258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014DD0500>]}
[0m11:53:11.892220 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:53:11.893218 [info ] [MainThread]: 
[0m11:53:11.899735 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:53:11.900691 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m11:53:11.900691 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m11:53:11.908980 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m11:53:11.912019 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:53:11.913015 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:53:11.913015 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m11:53:11.914013 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m11:53:11.917159 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m11:53:11.919821 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:53:11.919821 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:53:11.920815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m11:53:11.920815 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m11:53:11.922810 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m11:53:11.924739 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:53:11.924739 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m11:53:11.924739 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m11:53:11.925734 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m11:53:11.926929 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m11:53:11.928923 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m11:53:11.929921 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:53:11.929921 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m11:53:11.931929 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m11:53:11.931929 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:53:11.935906 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:53:11.938066 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:53:11.955271 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:12.282721 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:53:12.284122 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN "2024-11-28 11:50:00" AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m11:53:12.728727 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:8d63419a-4680-4149-a236-04d8f88392c7&page=queryresults
[0m11:53:15.655982 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290131781D0>]}
[0m11:53:15.657218 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (0.0 rows, 758.0 Bytes processed)[0m in 3.73s]
[0m11:53:15.658215 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:53:15.659212 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:53:15.659212 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m11:53:15.660210 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m11:53:15.660210 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:53:15.663202 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:53:15.664209 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:53:15.667194 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:15.913470 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:53:15.914587 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-11-28 11:50:00' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:53:16.533135 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:3103761f-15ab-400a-8b91-27af5b6086df&page=queryresults
[0m11:53:18.847578 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014CAAAB0>]}
[0m11:53:18.848574 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (0.0 rows, 1.8 KiB processed)[0m in 3.19s]
[0m11:53:18.849555 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:53:18.850568 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m11:53:18.850568 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m11:53:18.851535 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m11:53:18.852533 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m11:53:18.854527 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m11:53:18.855524 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m11:53:18.858517 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:19.099900 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m11:53:19.100894 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-11-28 11:50:00' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:53:19.569593 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:cf37ecee-bcb2-4cf3-9287-29b135abbd92&page=queryresults
[0m11:53:22.166799 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014EA7980>]}
[0m11:53:22.166799 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (0.0 rows, 1.1 KiB processed)[0m in 3.32s]
[0m11:53:22.168137 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m11:53:22.169134 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m11:53:22.169134 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m11:53:22.170138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m11:53:22.171164 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m11:53:22.182336 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m11:53:22.183350 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m11:53:22.217083 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:22.502639 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m11:53:22.504904 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_adresse 
            ORDER BY transaction_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m11:53:23.007285 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:816c1eaf-998f-49d2-94cf-7959dc436cee&page=queryresults
[0m11:53:25.502372 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014DD08F0>]}
[0m11:53:25.503369 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (0.0 rows, 73.0 Bytes processed)[0m in 3.33s]
[0m11:53:25.504346 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m11:53:25.505382 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m11:53:25.505382 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m11:53:25.506354 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m11:53:25.506354 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m11:53:25.510583 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:53:25.512578 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m11:53:25.516839 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:25.749381 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:53:25.750378 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m11:53:26.389052 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:6bc580f7-9144-4669-bdc5-b2eeda85257e&page=queryresults
[0m11:53:28.668437 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014A83B30>]}
[0m11:53:28.669435 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (0.0 rows, 10.0 Bytes processed)[0m in 3.16s]
[0m11:53:28.670431 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m11:53:28.670431 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m11:53:28.671429 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m11:53:28.672576 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m11:53:28.672576 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m11:53:28.676778 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:53:28.679792 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m11:53:28.682761 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:29.063656 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:53:29.064623 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m11:53:29.613327 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:b5e37c3a-062f-463e-9a2a-5bec5b7d32ad&page=queryresults
[0m11:53:31.545296 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290148438C0>]}
[0m11:53:31.546293 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (0.0 rows, 17.0 Bytes processed)[0m in 2.87s]
[0m11:53:31.547510 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m11:53:31.547510 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:53:31.548509 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m11:53:31.549088 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m11:53:31.549088 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:53:31.554077 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:53:31.555077 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:53:31.558448 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:31.940476 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:53:31.941472 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
        ROW_NUMBER() OVER (
            PARTITION BY fournisseur_nom, fournisseur_contact, fournisseur_adresse 
            ORDER BY provisionnement_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base
WHERE rn = 1

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m11:53:32.420141 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:7a748182-794d-4449-be23-d08873a08057&page=queryresults
[0m11:53:34.604181 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014C90D70>]}
[0m11:53:34.605180 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (0.0 rows, 56.0 Bytes processed)[0m in 3.06s]
[0m11:53:34.606154 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:53:34.607406 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m11:53:34.607406 [info ] [Thread-1 (]: 8 of 13 START sql incremental model dataWarehouse.d_Materiaux .................. [RUN]
[0m11:53:34.608384 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m11:53:34.608384 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m11:53:34.613408 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:53:34.614369 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m11:53:34.617564 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:34.847390 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:53:34.848420 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT materiaux_nom AS MateriauxNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS MateriauxID,
    MateriauxNom,
    null AS valeurEstimee
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` AS target
        WHERE target.MateriauxNom = base.MateriauxNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)
    values
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)


    
[0m11:53:35.337749 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4df717b0-f510-47d9-ad0a-d139aae5dd70&page=queryresults
[0m11:53:37.555305 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014EF37D0>]}
[0m11:53:37.556333 [info ] [Thread-1 (]: 8 of 13 OK created sql incremental model dataWarehouse.d_Materiaux ............. [[32mMERGE (0.0 rows, 90.0 Bytes processed)[0m in 2.95s]
[0m11:53:37.557557 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m11:53:37.557557 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m11:53:37.558587 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m11:53:37.559552 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m11:53:37.559552 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m11:53:37.564539 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m11:53:37.565539 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m11:53:37.569131 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:37.822846 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m11:53:37.823843 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`)


    
[0m11:53:38.253266 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:206667b8-bbc7-43e6-b7b0-1ca1cbea176a&page=queryresults
[0m11:53:40.738274 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014EF36E0>]}
[0m11:53:40.739274 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (0.0 rows, 192.0 Bytes processed)[0m in 3.18s]
[0m11:53:40.740232 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m11:53:40.740232 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m11:53:40.741230 [info ] [Thread-1 (]: 10 of 13 START sql incremental model dataWarehouse.d_Produit ................... [RUN]
[0m11:53:40.741883 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m11:53:40.742921 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m11:53:40.745876 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m11:53:40.748054 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m11:53:40.752078 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:40.999148 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m11:53:41.001110 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Produit` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        distinct produit_nom AS ProduitNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ProduitID,
    ProduitNom,
    null AS CoutdeRevient
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` AS target
        WHERE target.ProduitNom = base.ProduitNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)
    values
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)


    
[0m11:53:41.492279 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:d4b7c6d5-ad33-4c44-9a93-34e95aecad08&page=queryresults
[0m11:53:43.373152 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014DC1AC0>]}
[0m11:53:43.374151 [info ] [Thread-1 (]: 10 of 13 OK created sql incremental model dataWarehouse.d_Produit .............. [[32mMERGE (0.0 rows, 48.0 Bytes processed)[0m in 2.63s]
[0m11:53:43.375116 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m11:53:43.375116 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:53:43.376138 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m11:53:43.377111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m11:53:43.377111 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:53:43.382390 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:53:43.384366 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:53:43.387635 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:43.605335 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:53:43.606307 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m11:53:43.962445 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:380fd596-5906-4d0c-971e-46b74f383bb1&page=queryresults
[0m11:53:46.161253 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014EE4A40>]}
[0m11:53:46.162251 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (0.0 rows, 1012.0 Bytes processed)[0m in 2.78s]
[0m11:53:46.163225 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:53:46.164222 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:53:46.164222 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m11:53:46.165026 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m11:53:46.165026 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m11:53:46.184360 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:53:46.185357 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m11:53:46.190697 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:46.425059 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:53:46.427029 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m11:53:46.915708 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:36e36a93-20d6-4eb0-ad4f-82c75aa71ce1&page=queryresults
[0m11:53:49.374328 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014EFAE40>]}
[0m11:53:49.375324 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (0.0 rows, 1.0 KiB processed)[0m in 3.21s]
[0m11:53:49.376285 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:53:49.377430 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m11:53:49.377634 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m11:53:49.378633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m11:53:49.378633 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m11:53:49.382623 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m11:53:49.384624 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m11:53:49.389124 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:49.653387 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m11:53:49.654348 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m11:53:50.076373 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:bbf18518-875f-4ce5-9835-19f3e1e9839b&page=queryresults
[0m11:53:52.559350 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3028463-b158-41c6-8abc-429261eeffe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014EFAE40>]}
[0m11:53:52.560348 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (0.0 rows, 1.0 KiB processed)[0m in 3.18s]
[0m11:53:52.561345 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m11:53:52.563142 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:53:52.563142 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m11:53:52.563142 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m11:53:52.563142 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m11:53:52.564139 [info ] [MainThread]: 
[0m11:53:52.565138 [info ] [MainThread]: Finished running 3 table models, 10 incremental models in 0 hours 0 minutes and 41.37 seconds (41.37s).
[0m11:53:52.569127 [debug] [MainThread]: Command end result
[0m11:53:52.604525 [info ] [MainThread]: 
[0m11:53:52.605495 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:53:52.606492 [info ] [MainThread]: 
[0m11:53:52.607748 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m11:53:52.609119 [debug] [MainThread]: Command `dbt run` succeeded at 11:53:52.608117 after 45.76 seconds
[0m11:53:52.610115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002907E8795B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014D00860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029014AA3E00>]}
[0m11:53:52.610115 [debug] [MainThread]: Flushing usage events
[0m12:01:02.045253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBDE8DC3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBE11F3080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBE11F2480>]}


============================== 12:01:02.050292 | cd2069da-f7cc-4a12-bc8f-2aff10328b4c ==============================
[0m12:01:02.050292 [info ] [MainThread]: Running with dbt=1.8.9
[0m12:01:02.051301 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:01:03.548873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF1B7EDE0>]}
[0m12:01:03.606210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF26337D0>]}
[0m12:01:03.607209 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m12:01:03.881563 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m12:01:04.073336 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m12:01:04.074331 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\staging\stg_CommandeFournisseur.sql
[0m12:01:04.074331 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\staging\stg_Production.sql
[0m12:01:04.075329 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\staging\stg_CommandeClient.sql
[0m12:01:04.321271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3D500E0>]}
[0m12:01:04.435512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBDEF621B0>]}
[0m12:01:04.436509 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m12:01:04.437823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3DE2A80>]}
[0m12:01:04.439849 [info ] [MainThread]: 
[0m12:01:04.440861 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:01:04.447440 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m12:01:04.448465 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:01:04.826163 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m12:01:04.826163 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:01:05.071987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF37D7DA0>]}
[0m12:01:05.072953 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:01:05.073882 [info ] [MainThread]: 
[0m12:01:05.078243 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m12:01:05.079262 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m12:01:05.080229 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m12:01:05.090353 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m12:01:05.091353 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m12:01:05.092325 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m12:01:05.092325 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m12:01:05.093322 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m12:01:05.094319 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m12:01:05.096331 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m12:01:05.096467 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m12:01:05.096467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m12:01:05.097506 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m12:01:05.098498 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m12:01:05.099498 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m12:01:05.100457 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m12:01:05.100457 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m12:01:05.101475 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m12:01:05.103451 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m12:01:05.106448 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m12:01:05.107534 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:01:05.107660 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m12:01:05.108658 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m12:01:05.108658 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:01:05.111688 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m12:01:05.112683 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:01:05.130255 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:05.480224 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m12:01:05.481217 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN "2024-11-28 10:50:00" AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m12:01:05.924572 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:f254aaad-7e11-486b-a962-bd8ec84fb5a6&page=queryresults
[0m12:01:08.880508 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF1209790>]}
[0m12:01:08.881506 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (3.0 rows, 758.0 Bytes processed)[0m in 3.77s]
[0m12:01:08.882468 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:01:08.883464 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:01:08.883464 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m12:01:08.884522 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m12:01:08.884522 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:01:08.887922 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m12:01:08.889884 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:01:08.893874 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:09.139209 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m12:01:09.141164 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-11-28 10:50:00' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m12:01:09.716155 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:9887d8bc-e485-480a-9080-6129b794d313&page=queryresults
[0m12:01:12.350614 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3D58A40>]}
[0m12:01:12.350614 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (7.0 rows, 1.8 KiB processed)[0m in 3.47s]
[0m12:01:12.352578 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:01:12.352578 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m12:01:12.353575 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m12:01:12.354576 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m12:01:12.354576 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m12:01:12.357806 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m12:01:12.358804 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m12:01:12.361837 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:12.631657 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m12:01:12.633622 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-11-28 10:50:00' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m12:01:13.035805 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:dbcd7daf-19c3-4c48-a95f-fc5acbc01a37&page=queryresults
[0m12:01:15.671541 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF224D2B0>]}
[0m12:01:15.671541 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (3.0 rows, 1.1 KiB processed)[0m in 3.32s]
[0m12:01:15.673542 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m12:01:15.673542 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m12:01:15.674532 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m12:01:15.675332 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m12:01:15.675332 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m12:01:15.687858 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m12:01:15.688894 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m12:01:15.724658 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:16.055989 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m12:01:16.056953 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_adresse 
            ORDER BY transaction_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m12:01:16.562053 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:dbd0eae5-c4fb-4b31-8a6c-4f50c989e2dc&page=queryresults
[0m12:01:18.804431 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3E79D60>]}
[0m12:01:18.805427 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (1.0 rows, 213.0 Bytes processed)[0m in 3.13s]
[0m12:01:18.806394 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m12:01:18.807633 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m12:01:18.807633 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m12:01:18.808621 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m12:01:18.808621 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m12:01:18.812618 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m12:01:18.814610 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m12:01:18.820641 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:19.107944 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m12:01:19.108937 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m12:01:19.466077 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:0dbd7c8c-b809-4ee3-b6f2-f56cfc395484&page=queryresults
[0m12:01:21.649586 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3F44590>]}
[0m12:01:21.649586 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (1.0 rows, 27.0 Bytes processed)[0m in 2.84s]
[0m12:01:21.650554 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m12:01:21.651583 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m12:01:21.652543 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m12:01:21.653541 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m12:01:21.653541 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m12:01:21.657529 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m12:01:21.658527 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m12:01:21.663512 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:21.920498 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m12:01:21.921459 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m12:01:22.417338 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:48ac563a-77d2-49b4-8d25-42a1518bb678&page=queryresults
[0m12:01:24.666533 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3D786E0>]}
[0m12:01:24.667776 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (1.0 rows, 366.0 Bytes processed)[0m in 3.01s]
[0m12:01:24.668775 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m12:01:24.669773 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:01:24.669773 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m12:01:24.670809 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m12:01:24.671808 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:01:24.675798 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m12:01:24.677004 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:01:24.680033 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:25.023760 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m12:01:25.025714 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
        ROW_NUMBER() OVER (
            PARTITION BY fournisseur_nom, fournisseur_contact, fournisseur_adresse 
            ORDER BY provisionnement_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base
WHERE rn = 1

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m12:01:25.364849 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:cece1856-6df1-4681-9800-0f79c3df4987&page=queryresults
[0m12:01:27.727082 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF38D8E00>]}
[0m12:01:27.728393 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (2.0 rows, 380.0 Bytes processed)[0m in 3.06s]
[0m12:01:27.729390 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:01:27.729390 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m12:01:27.730432 [info ] [Thread-1 (]: 8 of 13 START sql incremental model dataWarehouse.d_Materiaux .................. [RUN]
[0m12:01:27.731389 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m12:01:27.731389 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m12:01:27.736374 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m12:01:27.738702 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m12:01:27.741716 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:28.126585 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m12:01:28.127770 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT materiaux_nom AS MateriauxNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS MateriauxID,
    MateriauxNom,
    null AS valeurEstimee
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` AS target
        WHERE target.MateriauxNom = base.MateriauxNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)
    values
        (`MateriauxID`, `MateriauxNom`, `ValeurEstimee`)


    
[0m12:01:28.607654 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:b815f5ce-9d40-4e1c-ac2c-4461db594126&page=queryresults
[0m12:01:31.134547 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3FDA4B0>]}
[0m12:01:31.134547 [info ] [Thread-1 (]: 8 of 13 OK created sql incremental model dataWarehouse.d_Materiaux ............. [[32mMERGE (3.0 rows, 174.0 Bytes processed)[0m in 3.40s]
[0m12:01:31.135650 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m12:01:31.136689 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m12:01:31.136689 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m12:01:31.137970 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m12:01:31.139003 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m12:01:31.143956 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m12:01:31.145964 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m12:01:31.149257 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:31.443233 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m12:01:31.444195 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`)


    
[0m12:01:32.394076 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:817986bb-1c85-401e-9dcd-c5668b7fc069&page=queryresults
[0m12:01:34.599089 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF387BCE0>]}
[0m12:01:34.599089 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (4.0 rows, 320.0 Bytes processed)[0m in 3.46s]
[0m12:01:34.601048 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m12:01:34.601048 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m12:01:34.602080 [info ] [Thread-1 (]: 10 of 13 START sql incremental model dataWarehouse.d_Produit ................... [RUN]
[0m12:01:34.602680 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m12:01:34.602680 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m12:01:34.606672 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m12:01:34.608668 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m12:01:34.613689 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:34.963912 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m12:01:34.964906 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Produit` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        distinct produit_nom AS ProduitNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ProduitID,
    ProduitNom,
    null AS CoutdeRevient
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` AS target
        WHERE target.ProduitNom = base.ProduitNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)
    values
        (`ProduitID`, `ProduitNom`, `CoutdeRevient`)


    
[0m12:01:35.483527 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:d7c0b1d1-a4bc-481e-9372-ed145a506659&page=queryresults
[0m12:01:37.721903 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3FDA6F0>]}
[0m12:01:37.722901 [info ] [Thread-1 (]: 10 of 13 OK created sql incremental model dataWarehouse.d_Produit .............. [[32mMERGE (0.0 rows, 99.0 Bytes processed)[0m in 3.12s]
[0m12:01:37.723863 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m12:01:37.724896 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:01:37.724896 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m12:01:37.725856 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m12:01:37.725856 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:01:37.730077 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m12:01:37.732080 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:01:37.737056 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:38.001754 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m12:01:38.002714 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m12:01:38.457383 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:cc40ba28-b48c-4871-9ca6-334215e79784&page=queryresults
[0m12:01:40.962221 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3FCAB10>]}
[0m12:01:40.963212 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (7.0 rows, 2.1 KiB processed)[0m in 3.24s]
[0m12:01:40.964190 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:01:40.965225 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m12:01:40.965225 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m12:01:40.966188 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m12:01:40.967239 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m12:01:40.986629 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m12:01:40.987867 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m12:01:40.993887 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:41.240514 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m12:01:41.241511 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m12:01:41.593587 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:5d8a96cb-3b64-47c8-be92-8f901d4d270e&page=queryresults
[0m12:01:44.067477 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3FCDF40>]}
[0m12:01:44.068514 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (3.0 rows, 1.6 KiB processed)[0m in 3.10s]
[0m12:01:44.069471 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m12:01:44.070469 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m12:01:44.070469 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m12:01:44.071467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m12:01:44.071467 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m12:01:44.076454 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m12:01:44.077453 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m12:01:44.082437 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:44.315878 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m12:01:44.316871 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m12:01:44.823990 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:88e25f0c-c7fa-46f7-994b-cc93c8fc2ed8&page=queryresults
[0m12:01:47.036179 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd2069da-f7cc-4a12-bc8f-2aff10328b4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3FCD640>]}
[0m12:01:47.037181 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (3.0 rows, 1.7 KiB processed)[0m in 2.96s]
[0m12:01:47.038498 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m12:01:47.039502 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:01:47.039502 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m12:01:47.040531 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m12:01:47.040531 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m12:01:47.040531 [info ] [MainThread]: 
[0m12:01:47.041532 [info ] [MainThread]: Finished running 3 table models, 10 incremental models in 0 hours 0 minutes and 42.60 seconds (42.60s).
[0m12:01:47.045480 [debug] [MainThread]: Command end result
[0m12:01:47.084045 [info ] [MainThread]: 
[0m12:01:47.085017 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:01:47.086012 [info ] [MainThread]: 
[0m12:01:47.086012 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m12:01:47.087275 [debug] [MainThread]: Command `dbt run` succeeded at 12:01:47.087275 after 45.19 seconds
[0m12:01:47.088300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBE10ED9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBE03784A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBF3D509E0>]}
[0m12:01:47.088300 [debug] [MainThread]: Flushing usage events
[0m12:36:47.421139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF71340E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF71341250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF71342120>]}


============================== 12:36:47.429331 | 9a232ab6-def5-4796-b01b-9e11de19e0f8 ==============================
[0m12:36:47.429331 [info ] [MainThread]: Running with dbt=1.8.9
[0m12:36:47.430330 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt docs generate', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:36:49.886979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a232ab6-def5-4796-b01b-9e11de19e0f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF01CE2270>]}
[0m12:36:49.943365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a232ab6-def5-4796-b01b-9e11de19e0f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF026D2F30>]}
[0m12:36:49.944363 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m12:36:50.237374 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m12:36:50.423986 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:36:50.424984 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:36:50.457637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a232ab6-def5-4796-b01b-9e11de19e0f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF01D92660>]}
[0m12:36:50.489387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a232ab6-def5-4796-b01b-9e11de19e0f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF03E268A0>]}
[0m12:36:50.490385 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m12:36:50.490385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a232ab6-def5-4796-b01b-9e11de19e0f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF711F62D0>]}
[0m12:36:50.493376 [info ] [MainThread]: 
[0m12:36:50.494374 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:36:50.499361 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m12:36:50.500963 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:36:50.881378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a232ab6-def5-4796-b01b-9e11de19e0f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF03AB8650>]}
[0m12:36:50.882376 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:36:50.883383 [info ] [MainThread]: 
[0m12:36:50.888362 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m12:36:50.889360 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m12:36:50.889360 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m12:36:50.903490 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m12:36:50.905485 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m12:36:50.905485 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m12:36:50.906482 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m12:36:50.906482 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m12:36:50.909723 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m12:36:50.912694 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m12:36:50.913726 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m12:36:50.913726 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m12:36:50.914687 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m12:36:50.915685 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m12:36:50.917809 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m12:36:50.917809 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m12:36:50.918807 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m12:36:50.918807 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m12:36:50.920802 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m12:36:50.922799 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m12:36:50.922799 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:36:50.923795 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m12:36:50.923795 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:36:50.928249 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m12:36:50.929281 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:36:50.930249 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m12:36:50.931278 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:36:50.931748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m12:36:50.931748 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:36:50.934742 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m12:36:50.935739 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:36:50.936737 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m12:36:50.936737 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m12:36:50.937854 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m12:36:50.937854 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m12:36:50.940882 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m12:36:50.942859 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m12:36:50.943840 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m12:36:50.943840 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m12:36:50.944837 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m12:36:50.944837 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m12:36:50.954810 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m12:36:50.955807 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m12:36:50.955807 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m12:36:50.956805 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m12:36:50.957801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m12:36:50.957801 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m12:36:50.961827 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m12:36:50.962787 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m12:36:50.962787 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m12:36:50.963784 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m12:36:50.963784 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m12:36:50.964781 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m12:36:50.967773 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m12:36:50.968771 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m12:36:50.969768 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m12:36:50.969768 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:36:50.970767 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m12:36:50.970767 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:36:50.974754 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m12:36:50.975752 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:36:50.975752 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m12:36:50.976969 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m12:36:50.976969 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m12:36:50.977993 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m12:36:50.980984 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m12:36:50.981996 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m12:36:50.982954 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m12:36:50.983987 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m12:36:50.983987 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m12:36:50.983987 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m12:36:50.989269 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m12:36:50.990231 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m12:36:50.991229 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m12:36:50.991229 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m12:36:50.992254 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m12:36:50.992254 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m12:36:50.996258 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m12:36:50.997538 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m12:36:50.997538 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m12:36:50.998573 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:36:50.998573 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m12:36:50.999571 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:36:51.002524 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m12:36:51.004554 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:36:51.005517 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m12:36:51.005517 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m12:36:51.005517 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m12:36:51.006844 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m12:36:51.022701 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m12:36:51.023670 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m12:36:51.023670 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m12:36:51.024688 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m12:36:51.025655 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m12:36:51.025655 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m12:36:51.030023 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m12:36:51.030023 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m12:36:51.030984 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m12:36:51.032976 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:36:51.032976 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m12:36:51.032976 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m12:36:51.035968 [debug] [MainThread]: Command end result
[0m12:36:51.528264 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m12:36:51.528264 [info ] [MainThread]: Building catalog
[0m12:36:51.536259 [debug] [ThreadPool]: Acquiring new bigquery connection 'projet-bi-isen.information_schema'
[0m12:36:51.558695 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:36:51.561721 [debug] [ThreadPool]: On projet-bi-isen.information_schema: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "connection_name": "projet-bi-isen.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `projet-bi-isen`.`dataWarehouse`.__TABLES__ tables
    left join `projet-bi-isen`.`dataWarehouse`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `projet-bi-isen`.`dataWarehouse`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('f_Production')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('f_CommandeClient')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('d_Entrepot')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('d_Produit')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('f_CommandeFournisseur')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('d_Client')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('d_Date')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('d_FournisseurDetails')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('stg_CommandeClient')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('d_Livreur')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('d_Materiaux')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('stg_CommandeFournisseur')
                            ) or (
                                upper(table_schema) = upper('dataWarehouse')
                            and upper(table_name) = upper('stg_Production')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `projet-bi-isen`.`dataWarehouse`.INFORMATION_SCHEMA.COLUMNS columns
    join `projet-bi-isen`.`dataWarehouse`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m12:36:51.967585 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:e8846f7b-de01-475c-af08-6bad2110251e&page=queryresults
[0m12:36:54.167125 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:36:54.169145 [debug] [ThreadPool]: On projet-bi-isen.information_schema: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "connection_name": "projet-bi-isen.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `projet-bi-isen`.`ODS`.__TABLES__ tables
    left join `projet-bi-isen`.`ODS`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `projet-bi-isen`.`ODS`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('ODS')
                            and upper(table_name) = upper('f_listeProduction')
                            ) or (
                                upper(table_schema) = upper('ODS')
                            and upper(table_name) = upper('f_commandeInternet')
                            ) or (
                                upper(table_schema) = upper('ODS')
                            and upper(table_name) = upper('f_approvisionnement')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `projet-bi-isen`.`ODS`.INFORMATION_SCHEMA.COLUMNS columns
    join `projet-bi-isen`.`ODS`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m12:36:54.537593 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:027ab1df-b699-44e7-a6dc-b7ca63fe303a&page=queryresults
[0m12:36:56.573107 [info ] [MainThread]: Catalog written to C:\Users\Moi\Documents\M2\BI\dbt\dbtProjetBi\alimentationDatawarehouse\target\catalog.json
[0m12:36:56.575193 [debug] [MainThread]: Command `dbt docs generate` succeeded at 12:36:56.575193 after 9.29 seconds
[0m12:36:56.575193 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m12:36:56.576194 [debug] [MainThread]: Connection 'projet-bi-isen.information_schema' was properly closed.
[0m12:36:56.576194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF7053CBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF70AC92E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF70ACB530>]}
[0m12:36:56.576194 [debug] [MainThread]: Flushing usage events
[0m12:37:13.342699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018226C79A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018226C7B1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018226C7AFC0>]}


============================== 12:37:13.347890 | 8a6d98ea-0bd6-4d67-b703-38dccabd75a2 ==============================
[0m12:37:13.347890 [info ] [MainThread]: Running with dbt=1.8.9
[0m12:37:13.347890 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt docs serve', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:37:14.488018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a6d98ea-0bd6-4d67-b703-38dccabd75a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018237FE73B0>]}
[0m12:37:14.545727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a6d98ea-0bd6-4d67-b703-38dccabd75a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182392A5580>]}
[0m11:34:06.552747 [error] [MainThread]: Encountered an error:

[0m11:34:07.128754 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\site-packages\dbt\cli\requires.py", line 138, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\site-packages\dbt\cli\requires.py", line 101, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\site-packages\dbt\cli\requires.py", line 247, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\site-packages\dbt\cli\requires.py", line 294, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\site-packages\dbt\cli\main.py", line 303, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\site-packages\dbt\task\docs\serve.py", line 29, in run
    httpd.serve_forever()
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\socketserver.py", line 235, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\selectors.py", line 323, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Moi\AppData\Local\Programs\Python\Python312\Lib\selectors.py", line 314, in _select
    r, w, x = select.select(r, w, w, timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m11:34:07.166331 [debug] [MainThread]: Command `dbt docs serve` failed at 11:34:07.162706 after 82613.95 seconds
[0m11:34:07.177978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018226A8FA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018239308860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018226E25760>]}
[0m11:34:07.185170 [debug] [MainThread]: Flushing usage events
[0m11:34:34.744606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDF5132C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDF511AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDF512C90>]}


============================== 11:34:34.751085 | a35c255c-efd4-44c6-b764-9da398302241 ==============================
[0m11:34:34.751085 [info ] [MainThread]: Running with dbt=1.8.9
[0m11:34:34.752083 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:34:41.252359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF212F200>]}
[0m11:34:41.310108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDEEF5D00>]}
[0m11:34:41.311112 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m11:34:41.821809 [debug] [MainThread]: checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1, vars: {}, profile: , target: , version: 1.8.9
[0m11:34:43.163445 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:34:43.164443 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_Produit.sql
[0m11:34:43.164443 [debug] [MainThread]: Partial parsing: updated file: alimentationDatawarehouse://models\dim\d_Materiaux.sql
[0m11:34:43.414354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF28B6330>]}
[0m11:34:43.549259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF28B1550>]}
[0m11:34:43.549259 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m11:34:43.550390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDF5121E0>]}
[0m11:34:43.553383 [info ] [MainThread]: 
[0m11:34:43.554381 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:34:43.560686 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m11:34:43.561697 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:43.971434 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m11:34:43.971434 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:44.232480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF283F170>]}
[0m11:34:44.232480 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:34:44.233478 [info ] [MainThread]: 
[0m11:34:44.239652 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:34:44.240434 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m11:34:44.241467 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m11:34:44.255422 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m11:34:44.259500 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m11:34:44.259500 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:34:44.260382 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m11:34:44.260382 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m11:34:44.262377 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m11:34:44.265369 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m11:34:44.265369 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:34:44.266389 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m11:34:44.266389 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m11:34:44.269358 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m11:34:44.271625 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m11:34:44.272621 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m11:34:44.272621 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m11:34:44.273618 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m11:34:44.276610 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m11:34:44.278605 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m11:34:44.278605 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:34:44.279698 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m11:34:44.279832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m11:34:44.280599 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:34:44.366508 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:34:44.368503 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:34:44.383634 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:44.702556 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m11:34:44.705235 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN "2024-11-28 10:50:00" AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m11:34:45.169918 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:d46ef75a-971f-4951-a941-ae8567ed07bc&page=queryresults
[0m11:34:47.507152 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF0CAB3B0>]}
[0m11:34:47.508147 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (263.0 rows, 28.8 KiB processed)[0m in 3.23s]
[0m11:34:47.509120 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m11:34:47.509120 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:34:47.510546 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m11:34:47.511138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m11:34:47.511138 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:34:47.515130 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:34:47.516133 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:34:47.519119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:47.804330 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m11:34:47.805393 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-11-28 10:50:00' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:34:48.197884 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:e5739689-ca98-48bb-a120-35c57ea28968&page=queryresults
[0m11:34:50.785632 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF28B0980>]}
[0m11:34:50.786628 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (160.0 rows, 22.3 KiB processed)[0m in 3.27s]
[0m11:34:50.787291 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m11:34:50.788326 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m11:34:50.788326 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m11:34:50.789523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m11:34:50.789523 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m11:34:50.792550 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m11:34:50.793522 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m11:34:50.797545 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:51.025442 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m11:34:51.026439 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-11-28 10:50:00' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m11:34:51.425712 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:026e9afb-b499-48a3-83ec-de4aba598e1a&page=queryresults
[0m11:34:53.711082 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF28B0B00>]}
[0m11:34:53.712078 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (175.0 rows, 15.4 KiB processed)[0m in 2.92s]
[0m11:34:53.713034 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m11:34:53.714079 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m11:34:53.714079 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m11:34:53.715033 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m11:34:53.715033 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m11:34:53.726029 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m11:34:53.727035 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m11:34:53.773067 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:54.079019 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m11:34:54.082885 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_adresse 
            ORDER BY transaction_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m11:34:54.539461 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:99230e85-e9cb-4a45-bf37-7334186f2b10&page=queryresults
[0m11:34:57.047016 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF29E0110>]}
[0m11:34:57.048014 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (78.0 rows, 11.5 KiB processed)[0m in 3.33s]
[0m11:34:57.049166 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m11:34:57.050165 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m11:34:57.050165 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m11:34:57.051162 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m11:34:57.052191 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m11:34:57.056149 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:34:57.057146 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m11:34:57.061619 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:57.303450 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m11:34:57.304442 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m11:34:57.768985 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:91fc191e-8cca-4c62-b4da-9061746ef16c&page=queryresults
[0m11:34:59.728130 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF29E0AD0>]}
[0m11:34:59.729389 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (4.0 rows, 2.0 KiB processed)[0m in 2.68s]
[0m11:34:59.730386 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m11:34:59.731384 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m11:34:59.731384 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m11:34:59.732382 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m11:34:59.733414 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m11:34:59.737369 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:34:59.739366 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m11:34:59.742728 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:59.976250 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m11:34:59.977211 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m11:35:00.516676 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:d265bd45-dc3b-4a9d-ad46-c2dac42e1024&page=queryresults
[0m11:35:02.704374 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF29E34A0>]}
[0m11:35:02.704374 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (1.0 rows, 9.8 KiB processed)[0m in 2.97s]
[0m11:35:02.706330 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m11:35:02.706330 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:02.707346 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m11:35:02.708382 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m11:35:02.708382 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:02.712680 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:35:02.713638 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:02.719861 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:02.966878 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m11:35:02.967873 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
        ROW_NUMBER() OVER (
            PARTITION BY fournisseur_nom, fournisseur_contact, fournisseur_adresse 
            ORDER BY provisionnement_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base
WHERE rn = 1

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m11:35:03.530139 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:ea2cc9dc-12c7-481b-ae31-69d0f8c12908&page=queryresults
[0m11:35:05.993910 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF28B0350>]}
[0m11:35:05.994907 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (5.0 rows, 6.3 KiB processed)[0m in 3.29s]
[0m11:35:05.997864 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m11:35:05.998904 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:05.998904 [info ] [Thread-1 (]: 8 of 13 START sql table model dataWarehouse.d_Materiaux ........................ [RUN]
[0m11:35:06.000137 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m11:35:06.001160 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:06.007149 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:35:06.008146 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:06.011418 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:06.278715 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m11:35:06.279860 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Materiaux`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        MateriauxID,
        PrixUnitaire,
        Quantite, 
    FROM `projet-bi-isen.dataWarehouse.f_CommandeFournisseur` 
),
aggregated_data AS (
    SELECT
        MateriauxID,
        SUM(PrixUnitaire * Quantite) AS somme_ponderee_prix,
        SUM(Quantite) AS somme_quantite
    FROM filtered_data
    GROUP BY MateriauxID
),
ste AS(
SELECT
  MateriauxID,
  CASE
      WHEN somme_quantite > 0 THEN ROUND(somme_ponderee_prix / somme_quantite,2)
      ELSE NULL
  END AS ValeurEstimee
FROM aggregated_data
),
existing_data AS (
    SELECT 
        mat.MateriauxID,
        mat.MateriauxNom,
        s.ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat  -- Table actuelle d_Materiaux
    LEFT JOIN ste s ON mat.MateriauxID = s.MateriauxID
),
new_data AS (
    SELECT
        GENERATE_UUID() AS MateriauxID,
        materiaux_nom AS MateriauxNom,
        prix_provision AS ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
unioned_data AS (
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM existing_data
    UNION ALL
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM new_data
    WHERE MateriauxNom NOT IN (SELECT MateriauxNom FROM existing_data)
)

SELECT 
    MateriauxID,
    MateriauxNom,
    ValeurEstimee
FROM unioned_data
    );
  
[0m11:35:06.702019 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:9919f43e-612c-46bb-bc9a-591e3eb3802a&page=queryresults
[0m11:35:09.891042 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF22A29F0>]}
[0m11:35:09.892037 [info ] [Thread-1 (]: 8 of 13 OK created sql table model dataWarehouse.d_Materiaux ................... [[32mCREATE TABLE (160.0 rows, 2.8 KiB processed)[0m in 3.89s]
[0m11:35:09.893051 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m11:35:09.894031 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m11:35:09.895028 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m11:35:09.895292 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m11:35:09.896292 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m11:35:09.900513 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m11:35:09.902509 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m11:35:09.906535 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:10.166219 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m11:35:10.167216 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`)


    
[0m11:35:10.648397 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:5edb05d1-70b5-4aec-a398-39418da7650a&page=queryresults
[0m11:35:13.102765 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF0D64050>]}
[0m11:35:13.103765 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (41.0 rows, 6.0 KiB processed)[0m in 3.21s]
[0m11:35:13.104761 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m11:35:13.105721 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m11:35:13.105721 [info ] [Thread-1 (]: 10 of 13 START sql table model dataWarehouse.d_Produit ......................... [RUN]
[0m11:35:13.106718 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m11:35:13.107716 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m11:35:13.110964 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m11:35:13.112924 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m11:35:13.116971 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:13.410156 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m11:35:13.411178 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Produit`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        prod.ProduitID,
        prod.QuantiteProduite,
        prod.MateriauxID,
        mat.ValeurEstimee AS PrixUnitaire,
        prod.QuantiteUtilise 
    FROM `projet-bi-isen.dataWarehouse.f_Production` prod
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat ON prod.MateriauxID = mat.MateriauxID
),
aggregated_data AS (
    SELECT
        ProduitID,
        SUM(QuantiteProduite) AS somme_quantite_produite,
        SUM(PrixUnitaire * QuantiteUtilise) AS somme_ponderee_prix,
        SUM(QuantiteUtilise) AS somme_quantite
    FROM filtered_data
    GROUP BY ProduitID
),
ste AS (
    SELECT
        ProduitID,
        CASE
            WHEN somme_quantite > 0 THEN ROUND((somme_ponderee_prix / somme_quantite) / somme_quantite_produite, 2)
            ELSE NULL
        END AS CoutdeRevient
    FROM aggregated_data
),
existing_data AS (
    SELECT 
        prod.ProduitID,
        prod.ProduitNom,
        s.CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` prod  -- Table actuelle d_Produit
    LEFT JOIN ste s ON prod.ProduitID = s.ProduitID
),
new_data AS (
    SELECT
        GENERATE_UUID() AS ProduitID,
        produit_nom AS ProduitNom,
        NULL AS CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
unioned_data AS (
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM existing_data
    UNION ALL
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM new_data
    -- Amélioration de la condition pour éviter les NULL avec NOT IN
    WHERE ProduitNom NOT IN (SELECT ProduitNom FROM existing_data WHERE ProduitNom IS NOT NULL)
)

SELECT 
    ProduitID,
    ProduitNom,
    CoutdeRevient
FROM unioned_data
    );
  
[0m11:35:13.811473 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4c6f1dd7-c254-4619-aa3e-8ad19a1115af&page=queryresults
[0m11:35:16.442188 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF2383EF0>]}
[0m11:35:16.442188 [info ] [Thread-1 (]: 10 of 13 OK created sql table model dataWarehouse.d_Produit .................... [[32mCREATE TABLE (175.0 rows, 10.2 KiB processed)[0m in 3.34s]
[0m11:35:16.444146 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m11:35:16.444146 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:16.445144 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m11:35:16.445853 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m11:35:16.446853 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:16.450842 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:35:16.456853 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:16.460113 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:16.694232 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m11:35:16.696431 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m11:35:17.127922 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:3992f2d4-f35d-4521-b195-3bc932e14309&page=queryresults
[0m11:35:19.874379 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF27A68D0>]}
[0m11:35:19.875341 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (984.0 rows, 22.1 KiB processed)[0m in 3.43s]
[0m11:35:19.876444 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m11:35:19.877445 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:19.878441 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m11:35:19.891022 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m11:35:19.891022 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:19.911329 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:35:19.912327 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:19.918311 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:20.161097 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m11:35:20.163054 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m11:35:20.611964 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:fcedbc31-8fbd-4591-8700-b0f50a18cfc3&page=queryresults
[0m11:35:22.839441 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF29CE600>]}
[0m11:35:22.840486 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (1.5k rows, 41.4 KiB processed)[0m in 2.95s]
[0m11:35:22.841479 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m11:35:22.842517 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m11:35:22.842517 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m11:35:22.843659 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m11:35:22.843659 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m11:35:22.848651 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m11:35:22.851144 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m11:35:22.855138 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:23.122167 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m11:35:23.123128 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m11:35:23.649345 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:78454d63-3e59-4344-bff6-fe5653a00036&page=queryresults
[0m11:35:26.219928 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a35c255c-efd4-44c6-b764-9da398302241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF29CD160>]}
[0m11:35:26.220958 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (5.0k rows, 33.6 KiB processed)[0m in 3.38s]
[0m11:35:26.222951 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m11:35:26.223918 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:35:26.224938 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m11:35:26.224938 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m11:35:26.224938 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m11:35:26.225935 [info ] [MainThread]: 
[0m11:35:26.225935 [info ] [MainThread]: Finished running 5 table models, 8 incremental models in 0 hours 0 minutes and 42.67 seconds (42.67s).
[0m11:35:26.230899 [debug] [MainThread]: Command end result
[0m11:35:26.273657 [info ] [MainThread]: 
[0m11:35:26.274659 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:35:26.275657 [info ] [MainThread]: 
[0m11:35:26.276655 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m11:35:26.277129 [debug] [MainThread]: Command `dbt run` succeeded at 11:35:26.277129 after 51.77 seconds
[0m11:35:26.278154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF23828D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF2382930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BF2381790>]}
[0m11:35:26.278154 [debug] [MainThread]: Flushing usage events
[0m14:28:57.729584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A8FDA7B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A90C05F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A90C06A50>]}


============================== 14:28:57.739074 | 7b1bab11-c682-4d65-9d01-1a7a10c49ede ==============================
[0m14:28:57.739074 [info ] [MainThread]: Running with dbt=1.8.9
[0m14:28:57.740466 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --vars {execution_date: 2024-12-01}', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:29:04.563735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A9045F140>]}
[0m14:29:04.621784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A90C64AD0>]}
[0m14:29:04.621784 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m14:29:05.090903 [debug] [MainThread]: checksum: 0a7db2ec7d53beccd4b5886de9d9aee72725a37baf7f8cf9a4ca0d4bb1123721, vars: {'execution_date': datetime.date(2024, 12, 1)}, profile: , target: , version: 1.8.9
[0m14:29:05.193593 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m14:29:05.195011 [debug] [MainThread]: previous checksum: 0a7db2ec7d53beccd4b5886de9d9aee72725a37baf7f8cf9a4ca0d4bb1123721, current checksum: 132857a23e9d07220246854f7763165174ce9b737cfc7867629ace4294e520b1
[0m14:29:05.195011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA36B0DD0>]}
[0m14:29:07.837230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA378F0E0>]}
[0m14:29:08.009805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3796F90>]}
[0m14:29:08.010808 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m14:29:08.011816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA38145C0>]}
[0m14:29:08.013969 [info ] [MainThread]: 
[0m14:29:08.015364 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:29:08.021351 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m14:29:08.021351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:29:08.475369 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m14:29:08.476402 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:29:08.718939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA1CF0B00>]}
[0m14:29:08.719907 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:29:08.720903 [info ] [MainThread]: 
[0m14:29:08.727839 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m14:29:08.728837 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m14:29:08.729836 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m14:29:08.740300 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m14:29:08.743290 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m14:29:08.743398 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m14:29:08.744396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m14:29:08.744396 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m14:29:08.746390 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m14:29:08.748386 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m14:29:08.749413 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m14:29:08.750380 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m14:29:08.750380 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m14:29:08.753371 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m14:29:08.754597 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m14:29:08.755596 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m14:29:08.755596 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m14:29:08.755596 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m14:29:08.757619 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m14:29:08.759188 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m14:29:08.760187 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:29:08.760187 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m14:29:08.761220 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m14:29:08.761220 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:29:08.764425 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m14:29:08.767384 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:29:08.784948 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:09.055824 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m14:29:09.057781 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN '2024-12-01' AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m14:29:09.518874 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:0dc3dc1f-4506-4ef2-af0d-110310d26ae3&page=queryresults
[0m14:29:11.856510 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA1C68290>]}
[0m14:29:11.856510 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (145.0 rows, 15.9 KiB processed)[0m in 3.09s]
[0m14:29:11.857851 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:29:11.858852 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:29:11.859850 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m14:29:11.860179 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m14:29:11.861216 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:29:11.864299 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m14:29:11.865298 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:29:11.868325 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:12.121528 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m14:29:12.123737 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        DateLivraison AS date_livraison,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-12-01' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m14:29:12.560969 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:7e61d406-bca3-4a39-8351-3d0d635e7e11&page=queryresults
[0m14:29:14.860198 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3991D00>]}
[0m14:29:14.861196 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (70.0 rows, 10.3 KiB processed)[0m in 3.00s]
[0m14:29:14.862158 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:29:14.862158 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m14:29:14.863492 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m14:29:14.864499 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m14:29:14.864499 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m14:29:14.868483 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m14:29:14.869477 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m14:29:14.872470 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:15.115911 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m14:29:15.116906 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-12-01' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m14:29:15.553097 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:c8ad98ac-c990-4fd2-9fef-a8aa16f949b5&page=queryresults
[0m14:29:17.560729 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA37863F0>]}
[0m14:29:17.560729 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (84.0 rows, 7.3 KiB processed)[0m in 2.70s]
[0m14:29:17.562687 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m14:29:17.562687 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m14:29:17.563965 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m14:29:17.563965 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m14:29:17.564999 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m14:29:17.571977 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m14:29:17.573495 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m14:29:17.608213 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:17.904588 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m14:29:17.905587 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_contact 
            ORDER BY transaction_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m14:29:18.383154 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:2d428eb2-d773-432f-9dd7-a3a6d41db7f2&page=queryresults
[0m14:29:20.605082 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3A16F90>]}
[0m14:29:20.605082 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (65.0 rows, 6.4 KiB processed)[0m in 3.04s]
[0m14:29:20.606089 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m14:29:20.607077 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m14:29:20.608074 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m14:29:20.608074 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m14:29:20.609072 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m14:29:20.612064 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m14:29:20.614061 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m14:29:20.617084 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:20.863260 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m14:29:20.864515 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m14:29:21.398226 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:13a213c3-26b6-48de-a90f-b6fbc98112a4&page=queryresults
[0m14:29:23.314538 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA39F9400>]}
[0m14:29:23.315537 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (4.0 rows, 1.1 KiB processed)[0m in 2.71s]
[0m14:29:23.316534 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m14:29:23.316534 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m14:29:23.317532 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m14:29:23.318529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m14:29:23.318529 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m14:29:23.322519 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m14:29:23.324650 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m14:29:23.331396 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:23.560030 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m14:29:23.561027 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m14:29:24.158819 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:6a522dcb-ccf9-4b17-a38f-d4d89ca380b4&page=queryresults
[0m14:29:26.029930 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA32BBD40>]}
[0m14:29:26.030925 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (1.0 rows, 4.3 KiB processed)[0m in 2.71s]
[0m14:29:26.031905 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m14:29:26.032884 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:29:26.032884 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m14:29:26.034104 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m14:29:26.034104 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:29:26.038099 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m14:29:26.040091 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:29:26.044377 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:26.335179 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m14:29:26.337135 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
        ROW_NUMBER() OVER (
            PARTITION BY fournisseur_nom, fournisseur_contact, fournisseur_adresse 
            ORDER BY provisionnement_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base
WHERE rn = 1

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m14:29:26.850210 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:e454b117-2a3e-471f-b64b-703d4e15c399&page=queryresults
[0m14:29:28.819007 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA39FD0D0>]}
[0m14:29:28.819007 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (5.0 rows, 2.7 KiB processed)[0m in 2.78s]
[0m14:29:28.820190 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:29:28.821220 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m14:29:28.821220 [info ] [Thread-1 (]: 8 of 13 START sql table model dataWarehouse.d_Materiaux ........................ [RUN]
[0m14:29:28.822187 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m14:29:28.823302 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m14:29:28.826447 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m14:29:28.829437 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m14:29:28.833710 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:29.080548 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m14:29:29.081543 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Materiaux`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        MateriauxID,
        PrixUnitaire,
        Quantite, 
    FROM `projet-bi-isen.dataWarehouse.f_CommandeFournisseur` 
),
aggregated_data AS (
    SELECT
        MateriauxID,
        SUM(PrixUnitaire * Quantite) AS somme_ponderee_prix,
        SUM(Quantite) AS somme_quantite
    FROM filtered_data
    GROUP BY MateriauxID
),
ste AS(
    SELECT
    MateriauxID,
    CASE
        WHEN somme_quantite > 0 THEN ROUND(somme_ponderee_prix / somme_quantite,2)
        ELSE NULL
    END AS ValeurEstimee
    FROM aggregated_data
),
existing_data AS (
    SELECT 
        mat.MateriauxID,
        mat.MateriauxNom,
        s.ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat  -- Table actuelle d_Materiaux
    LEFT JOIN ste s ON mat.MateriauxID = s.MateriauxID
),
newDataValeruEstimee AS (
    SELECT
        materiaux_nom,
        ROUND(SUM(quantite_provisionnee*prix_provision)/SUM(quantite_provisionnee),2) AS ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
    GROUP BY materiaux_nom
),
new_data AS (
    SELECT
        GENERATE_UUID() AS MateriauxID,
        comf.materiaux_nom AS MateriauxNom,
        nd.ValeurEstimee AS ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur` comf
    LEFT JOIN newDataValeruEstimee nd ON comf.materiaux_nom = nd.materiaux_nom
    GROUP BY comf.materiaux_nom, nd.ValeurEstimee
),
unioned_data AS (
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM existing_data
    UNION ALL
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM new_data
    WHERE MateriauxNom NOT IN (SELECT MateriauxNom FROM existing_data)
)

SELECT 
    MateriauxID,
    MateriauxNom,
    ValeurEstimee
FROM unioned_data
    );
  
[0m14:29:29.475677 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:1848828d-cbae-4faa-92d3-34c6c5141d94&page=queryresults
[0m14:29:32.351540 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3E051F0>]}
[0m14:29:32.351540 [info ] [Thread-1 (]: 8 of 13 OK created sql table model dataWarehouse.d_Materiaux ................... [[32mCREATE TABLE (26.0 rows, 1.8 KiB processed)[0m in 3.53s]
[0m14:29:32.352541 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m14:29:32.353831 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m14:29:32.353831 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m14:29:32.354832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m14:29:32.355829 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m14:29:32.360814 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m14:29:32.364210 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m14:29:32.367235 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:32.632643 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m14:29:32.633931 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
    UNION ALL
    SELECT DISTINCT
        date_livraison AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours,
    TriDate AS DateViz
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`, `DateViz`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`, `DateViz`)


    
[0m14:29:33.106125 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:0ad362b5-01cc-46f3-aa01-fb87fe1974d0&page=queryresults
[0m14:29:35.206270 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3E06690>]}
[0m14:29:35.207268 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (36.0 rows, 3.5 KiB processed)[0m in 2.85s]
[0m14:29:35.208229 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m14:29:35.209264 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m14:29:35.209264 [info ] [Thread-1 (]: 10 of 13 START sql table model dataWarehouse.d_Produit ......................... [RUN]
[0m14:29:35.210466 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m14:29:35.210466 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m14:29:35.214747 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m14:29:35.216702 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m14:29:35.220716 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:35.612450 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m14:29:35.613644 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Produit`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        prod.ProduitID,
        prod.QuantiteProduite,
        prod.MateriauxID,
        mat.ValeurEstimee AS PrixUnitaire,
        prod.QuantiteUtilise 
    FROM `projet-bi-isen.dataWarehouse.f_Production` prod
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat ON prod.MateriauxID = mat.MateriauxID
),
aggregated_data AS (
    SELECT
        ProduitID,
        SUM(QuantiteProduite) AS somme_quantite_produite,
        SUM(PrixUnitaire * QuantiteUtilise) AS somme_ponderee_prix,
        SUM(QuantiteUtilise) AS somme_quantite
    FROM filtered_data
    GROUP BY ProduitID
),
ste AS (
    SELECT
        ProduitID,
        CASE
            WHEN somme_quantite > 0 THEN ROUND((somme_ponderee_prix / somme_quantite) / somme_quantite_produite, 2)
            ELSE NULL
        END AS CoutdeRevient
    FROM aggregated_data
),
existing_data AS (
    SELECT 
        prod.ProduitID,
        prod.ProduitNom,
        s.CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` prod  -- Table actuelle d_Produit
    LEFT JOIN ste s ON prod.ProduitID = s.ProduitID
),
newdataCoutDeRevient AS (
    SELECT
        prod.produit_nom,
        ROUND(SUM(mat.ValeurEstimee*prod.quantite_utilisee)/prod.quantite_produite,2) AS CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production` prod
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat ON prod.materiaux_utilises = mat.MateriauxNom
    GROUP BY produit_nom, prod.quantite_produite
),
new_data AS (
    SELECT
        GENERATE_UUID() AS ProduitID,
        prod.produit_nom AS ProduitNom,
        ncdt.CoutdeRevient AS CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production` prod
    LEFT JOIN newdataCoutDeRevient ncdt ON prod.produit_nom = ncdt.produit_nom
    GROUP BY prod.produit_nom, ncdt.CoutdeRevient
),
unioned_data AS (
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM existing_data
    UNION ALL
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM new_data
    -- Amélioration de la condition pour éviter les NULL avec NOT IN
    WHERE ProduitNom NOT IN (SELECT ProduitNom FROM existing_data WHERE ProduitNom IS NOT NULL)
)

SELECT 
    ProduitID,
    ProduitNom,
    CoutdeRevient
FROM unioned_data
    );
  
[0m14:29:36.048662 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:ad1e8d7e-3fdf-46f1-8b32-03c62be79e9e&page=queryresults
[0m14:29:39.036440 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3DF58E0>]}
[0m14:29:39.036440 [info ] [Thread-1 (]: 10 of 13 OK created sql table model dataWarehouse.d_Produit .................... [[32mCREATE TABLE (14.0 rows, 5.1 KiB processed)[0m in 3.82s]
[0m14:29:39.037438 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m14:29:39.038463 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:29:39.038463 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m14:29:39.039457 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m14:29:39.040455 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:29:39.044420 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m14:29:39.046415 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:29:39.048434 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:39.297432 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m14:29:39.300408 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        date_livraison AS DateLivraisonProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        dim_dateLivraison.DateID AS DateLivraisonProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_dateLivraison
        ON dim_dateLivraison.Annee = EXTRACT(YEAR FROM sd.DateLivraisonProvision)
        AND dim_dateLivraison.Mois = EXTRACT(MONTH FROM sd.DateLivraisonProvision)
        AND dim_dateLivraison.Jours = EXTRACT(DAY FROM sd.DateLivraisonProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    DateLivraisonProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `DateLivraisonProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `DateLivraisonProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m14:29:39.793385 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:69cf3b67-c821-40e3-85e6-b66b5164bb19&page=queryresults
[0m14:29:41.693473 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3DDE2D0>]}
[0m14:29:41.693606 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (70.0 rows, 9.4 KiB processed)[0m in 2.65s]
[0m14:29:41.694603 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:29:41.695601 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m14:29:41.695601 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m14:29:41.696599 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m14:29:41.696599 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m14:29:41.716179 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m14:29:41.717141 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m14:29:41.722162 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:41.989712 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m14:29:41.990710 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m14:29:42.416710 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:3f35d171-99b5-4c7b-b1a8-70e6b7f0c01d&page=queryresults
[0m14:29:46.054698 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA3E1C6B0>]}
[0m14:29:46.055695 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (145.0 rows, 20.8 KiB processed)[0m in 4.36s]
[0m14:29:46.056692 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m14:29:46.057689 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m14:29:46.058686 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m14:29:46.059683 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m14:29:46.060680 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m14:29:46.066806 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m14:29:46.068801 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m14:29:46.072790 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:46.326928 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m14:29:46.327929 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m14:29:46.812908 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:f5ce9bdb-5b5d-4995-be6b-06f2b8b148c9&page=queryresults
[0m14:29:48.725069 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b1bab11-c682-4d65-9d01-1a7a10c49ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A8D160500>]}
[0m14:29:48.725069 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (84.0 rows, 10.9 KiB processed)[0m in 2.67s]
[0m14:29:48.726418 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m14:29:48.728416 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:29:48.728416 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m14:29:48.728416 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m14:29:48.728416 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m14:29:48.729444 [info ] [MainThread]: 
[0m14:29:48.730459 [info ] [MainThread]: Finished running 5 table models, 8 incremental models in 0 hours 0 minutes and 40.71 seconds (40.71s).
[0m14:29:48.733574 [debug] [MainThread]: Command end result
[0m14:29:48.771694 [info ] [MainThread]: 
[0m14:29:48.772692 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:29:48.773937 [info ] [MainThread]: 
[0m14:29:48.773937 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m14:29:48.775324 [debug] [MainThread]: Command `dbt run` succeeded at 14:29:48.775324 after 51.26 seconds
[0m14:29:48.776323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA1C7E9F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A8FDA7B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A8FDA7710>]}
[0m14:29:48.776323 [debug] [MainThread]: Flushing usage events
[0m14:37:09.594566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8DB0C4A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8FAC9700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8CBC9280>]}


============================== 14:37:09.603210 | 03cad6d7-1e03-42eb-b754-fc31424e22e9 ==============================
[0m14:37:09.603210 [info ] [MainThread]: Running with dbt=1.8.9
[0m14:37:09.604433 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --vars {execution_date: 2024-12-02}', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:37:12.125457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8F64CB00>]}
[0m14:37:12.184335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA1864860>]}
[0m14:37:12.185336 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m14:37:12.464672 [debug] [MainThread]: checksum: e88dba9e2f09403cb99c6d862a8a24d4188893f218dc898d11a697b19a4fddc5, vars: {'execution_date': datetime.date(2024, 12, 2)}, profile: , target: , version: 1.8.9
[0m14:37:12.655357 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m14:37:12.656360 [debug] [MainThread]: previous checksum: e88dba9e2f09403cb99c6d862a8a24d4188893f218dc898d11a697b19a4fddc5, current checksum: 0a7db2ec7d53beccd4b5886de9d9aee72725a37baf7f8cf9a4ca0d4bb1123721
[0m14:37:12.657322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8FCCE600>]}
[0m14:37:13.989387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA2F75B80>]}
[0m14:37:14.140003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA2F94FB0>]}
[0m14:37:14.141003 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m14:37:14.141003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA3039130>]}
[0m14:37:14.144105 [info ] [MainThread]: 
[0m14:37:14.145105 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:37:14.152085 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m14:37:14.153084 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:37:14.567552 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m14:37:14.568575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:37:14.819911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8E09A8A0>]}
[0m14:37:14.820909 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:37:14.820909 [info ] [MainThread]: 
[0m14:37:14.826016 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m14:37:14.827015 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m14:37:14.828015 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m14:37:14.839979 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m14:37:14.841976 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m14:37:14.841976 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m14:37:14.842972 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m14:37:14.842972 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m14:37:14.845140 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m14:37:14.847134 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m14:37:14.848171 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m14:37:14.848171 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m14:37:14.849147 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m14:37:14.850160 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m14:37:14.852120 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m14:37:14.854115 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m14:37:14.854115 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m14:37:14.855112 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m14:37:14.858104 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m14:37:14.859102 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m14:37:14.860098 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:37:14.860098 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m14:37:14.861103 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m14:37:14.861103 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:37:14.864409 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m14:37:14.865404 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:37:14.884644 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:15.196938 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m14:37:15.198917 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN '2024-12-02' AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m14:37:15.799369 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:937e1f1d-b10a-4e77-9d9f-0e60a7696db0&page=queryresults
[0m14:37:18.451606 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA31B15B0>]}
[0m14:37:18.451606 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (118.0 rows, 28.8 KiB processed)[0m in 3.59s]
[0m14:37:18.452604 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:37:18.453598 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:37:18.454595 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m14:37:18.454930 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m14:37:18.454930 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:37:18.458924 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m14:37:18.459922 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:37:18.464224 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:18.759161 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m14:37:18.760170 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        DateLivraison AS date_livraison,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-12-02' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m14:37:19.173513 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4217e997-f01b-453b-ba5f-93b95f759520&page=queryresults
[0m14:37:21.191349 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA2FAB680>]}
[0m14:37:21.192346 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (90.0 rows, 23.6 KiB processed)[0m in 2.74s]
[0m14:37:21.193439 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:37:21.194437 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m14:37:21.194437 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m14:37:21.195433 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m14:37:21.196431 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m14:37:21.201419 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m14:37:21.204490 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m14:37:21.207702 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:21.485861 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m14:37:21.487857 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-12-02' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m14:37:21.886900 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:450f230d-8303-4cc7-8599-707f627db2e8&page=queryresults
[0m14:37:24.209202 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA2FC2480>]}
[0m14:37:24.209202 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (91.0 rows, 15.4 KiB processed)[0m in 3.01s]
[0m14:37:24.210652 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m14:37:24.211692 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m14:37:24.211692 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m14:37:24.212686 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m14:37:24.213950 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m14:37:24.220963 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m14:37:24.221961 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m14:37:24.276097 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:24.612809 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m14:37:24.615041 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_contact 
            ORDER BY transaction_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m14:37:25.087737 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:8e2300df-a214-4020-ab0b-d8b327b24fd6&page=queryresults
[0m14:37:27.286318 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA3226A50>]}
[0m14:37:27.287320 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (61.0 rows, 7.3 KiB processed)[0m in 3.07s]
[0m14:37:27.288278 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m14:37:27.289321 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m14:37:27.289321 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m14:37:27.290362 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m14:37:27.290362 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m14:37:27.294359 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m14:37:27.296350 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m14:37:27.301349 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:27.573661 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m14:37:27.574976 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m14:37:28.046425 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:1f98bb7c-dded-4486-999b-6aed0513166f&page=queryresults
[0m14:37:30.235468 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA32274D0>]}
[0m14:37:30.236468 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (0.0 rows, 1.0 KiB processed)[0m in 2.95s]
[0m14:37:30.237427 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m14:37:30.237427 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m14:37:30.238463 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m14:37:30.239439 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m14:37:30.239439 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m14:37:30.243412 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m14:37:30.244587 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m14:37:30.249604 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:30.520321 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m14:37:30.521319 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m14:37:31.155284 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:d05587be-1088-4cdc-a779-ee92ab1b4d94&page=queryresults
[0m14:37:33.050978 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA319A7B0>]}
[0m14:37:33.051972 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (0.0 rows, 5.6 KiB processed)[0m in 2.81s]
[0m14:37:33.052934 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m14:37:33.054189 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:37:33.054189 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m14:37:33.055216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m14:37:33.056227 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:37:33.060174 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m14:37:33.061172 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:37:33.065481 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:33.344464 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m14:37:33.345463 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
        ROW_NUMBER() OVER (
            PARTITION BY fournisseur_nom, fournisseur_contact, fournisseur_adresse 
            ORDER BY provisionnement_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base
WHERE rn = 1

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m14:37:33.807821 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:3b17b72d-7195-4208-be13-a9321cf51023&page=queryresults
[0m14:37:36.270371 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA32274D0>]}
[0m14:37:36.271369 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (0.0 rows, 3.7 KiB processed)[0m in 3.22s]
[0m14:37:36.272342 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:37:36.273666 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m14:37:36.273666 [info ] [Thread-1 (]: 8 of 13 START sql table model dataWarehouse.d_Materiaux ........................ [RUN]
[0m14:37:36.274664 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m14:37:36.274664 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m14:37:36.279652 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m14:37:36.280651 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m14:37:36.285217 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:36.545258 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m14:37:36.546253 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Materiaux`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        MateriauxID,
        PrixUnitaire,
        Quantite, 
    FROM `projet-bi-isen.dataWarehouse.f_CommandeFournisseur` 
),
aggregated_data AS (
    SELECT
        MateriauxID,
        SUM(PrixUnitaire * Quantite) AS somme_ponderee_prix,
        SUM(Quantite) AS somme_quantite
    FROM filtered_data
    GROUP BY MateriauxID
),
ste AS(
    SELECT
    MateriauxID,
    CASE
        WHEN somme_quantite > 0 THEN ROUND(somme_ponderee_prix / somme_quantite,2)
        ELSE NULL
    END AS ValeurEstimee
    FROM aggregated_data
),
existing_data AS (
    SELECT 
        mat.MateriauxID,
        mat.MateriauxNom,
        s.ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat  -- Table actuelle d_Materiaux
    LEFT JOIN ste s ON mat.MateriauxID = s.MateriauxID
),
newDataValeruEstimee AS (
    SELECT
        materiaux_nom,
        ROUND(SUM(quantite_provisionnee*prix_provision)/SUM(quantite_provisionnee),2) AS ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
    GROUP BY materiaux_nom
),
new_data AS (
    SELECT
        GENERATE_UUID() AS MateriauxID,
        comf.materiaux_nom AS MateriauxNom,
        nd.ValeurEstimee AS ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur` comf
    LEFT JOIN newDataValeruEstimee nd ON comf.materiaux_nom = nd.materiaux_nom
    GROUP BY comf.materiaux_nom, nd.ValeurEstimee
),
unioned_data AS (
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM existing_data
    UNION ALL
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM new_data
    WHERE MateriauxNom NOT IN (SELECT MateriauxNom FROM existing_data)
)

SELECT 
    MateriauxID,
    MateriauxNom,
    ValeurEstimee
FROM unioned_data
    );
  
[0m14:37:36.895736 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:025f8adf-ca73-4bd3-9a24-4898fe4bcb58&page=queryresults
[0m14:37:39.790343 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA3622630>]}
[0m14:37:39.791338 [info ] [Thread-1 (]: 8 of 13 OK created sql table model dataWarehouse.d_Materiaux ................... [[32mCREATE TABLE (29.0 rows, 7.2 KiB processed)[0m in 3.52s]
[0m14:37:39.792298 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m14:37:39.793334 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m14:37:39.793677 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m14:37:39.794675 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m14:37:39.794675 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m14:37:39.799665 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m14:37:39.800698 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m14:37:39.803944 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:40.066551 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m14:37:40.068545 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
    UNION ALL
    SELECT DISTINCT
        date_livraison AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours,
    TriDate AS DateViz
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`, `DateViz`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`, `DateViz`)


    
[0m14:37:40.479772 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:1b3565f4-a32a-4518-9903-2d0acfb4f6ab&page=queryresults
[0m14:37:42.653861 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA3621190>]}
[0m14:37:42.654894 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (6.0 rows, 4.6 KiB processed)[0m in 2.86s]
[0m14:37:42.655855 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m14:37:42.656853 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m14:37:42.656853 [info ] [Thread-1 (]: 10 of 13 START sql table model dataWarehouse.d_Produit ......................... [RUN]
[0m14:37:42.657850 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m14:37:42.657850 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m14:37:42.664096 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m14:37:42.665094 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m14:37:42.669082 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:42.950137 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m14:37:42.951094 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Produit`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        prod.ProduitID,
        prod.QuantiteProduite,
        prod.MateriauxID,
        mat.ValeurEstimee AS PrixUnitaire,
        prod.QuantiteUtilise 
    FROM `projet-bi-isen.dataWarehouse.f_Production` prod
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat ON prod.MateriauxID = mat.MateriauxID
),
aggregated_data AS (
    SELECT
        ProduitID,
        SUM(QuantiteProduite) AS somme_quantite_produite,
        SUM(PrixUnitaire * QuantiteUtilise) AS somme_ponderee_prix,
        SUM(QuantiteUtilise) AS somme_quantite
    FROM filtered_data
    GROUP BY ProduitID
),
ste AS (
    SELECT
        ProduitID,
        CASE
            WHEN somme_quantite > 0 THEN ROUND((somme_ponderee_prix / somme_quantite) / somme_quantite_produite, 2)
            ELSE NULL
        END AS CoutdeRevient
    FROM aggregated_data
),
existing_data AS (
    SELECT 
        prod.ProduitID,
        prod.ProduitNom,
        s.CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` prod  -- Table actuelle d_Produit
    LEFT JOIN ste s ON prod.ProduitID = s.ProduitID
),
newdataCoutDeRevient AS (
    SELECT
        prod.produit_nom,
        ROUND(SUM(mat.ValeurEstimee*prod.quantite_utilisee)/prod.quantite_produite,2) AS CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production` prod
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat ON prod.materiaux_utilises = mat.MateriauxNom
    GROUP BY produit_nom, prod.quantite_produite
),
new_data AS (
    SELECT
        GENERATE_UUID() AS ProduitID,
        prod.produit_nom AS ProduitNom,
        ncdt.CoutdeRevient AS CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production` prod
    LEFT JOIN newdataCoutDeRevient ncdt ON prod.produit_nom = ncdt.produit_nom
    GROUP BY prod.produit_nom, ncdt.CoutdeRevient
),
unioned_data AS (
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM existing_data
    UNION ALL
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM new_data
    -- Amélioration de la condition pour éviter les NULL avec NOT IN
    WHERE ProduitNom NOT IN (SELECT ProduitNom FROM existing_data WHERE ProduitNom IS NOT NULL)
)

SELECT 
    ProduitID,
    ProduitNom,
    CoutdeRevient
FROM unioned_data
    );
  
[0m14:37:43.445903 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:9918759e-5256-4adb-8d0a-80ef8ff516ea&page=queryresults
[0m14:37:46.304563 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA3202210>]}
[0m14:37:46.305595 [info ] [Thread-1 (]: 10 of 13 OK created sql table model dataWarehouse.d_Produit .................... [[32mCREATE TABLE (30.0 rows, 13.7 KiB processed)[0m in 3.65s]
[0m14:37:46.306609 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m14:37:46.307555 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:37:46.307555 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m14:37:46.308723 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m14:37:46.309759 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:37:46.313920 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m14:37:46.316924 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:37:46.319943 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:46.557628 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m14:37:46.559592 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        date_livraison AS DateLivraisonProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        dim_dateLivraison.DateID AS DateLivraisonProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_dateLivraison
        ON dim_dateLivraison.Annee = EXTRACT(YEAR FROM sd.DateLivraisonProvision)
        AND dim_dateLivraison.Mois = EXTRACT(MONTH FROM sd.DateLivraisonProvision)
        AND dim_dateLivraison.Jours = EXTRACT(DAY FROM sd.DateLivraisonProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    DateLivraisonProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `DateLivraisonProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `DateLivraisonProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m14:37:46.887828 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:8722abbf-1a70-48e5-90fd-8e2e38b59fc6&page=queryresults
[0m14:37:50.210200 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA3622ED0>]}
[0m14:37:50.211197 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (90.0 rows, 11.6 KiB processed)[0m in 3.90s]
[0m14:37:50.213041 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:37:50.213041 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m14:37:50.214333 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m14:37:50.214717 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m14:37:50.214717 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m14:37:50.234510 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m14:37:50.235544 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m14:37:50.241523 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:50.474524 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m14:37:50.475505 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m14:37:50.793283 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:05a0e01d-446d-4bf5-bcfa-d41ce86ead8b&page=queryresults
[0m14:37:52.975490 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA36443B0>]}
[0m14:37:52.976489 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (118.0 rows, 23.9 KiB processed)[0m in 2.76s]
[0m14:37:52.977450 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m14:37:52.978447 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m14:37:52.978447 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m14:37:52.979373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m14:37:52.979373 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m14:37:52.984627 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m14:37:52.985626 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m14:37:52.989645 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:53.378584 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m14:37:53.379551 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m14:37:53.861320 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:51f8f554-d3e8-4662-a48e-44fa25df6acf&page=queryresults
[0m14:37:56.057706 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03cad6d7-1e03-42eb-b754-fc31424e22e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBA36443E0>]}
[0m14:37:56.057706 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (91.0 rows, 12.9 KiB processed)[0m in 3.08s]
[0m14:37:56.058704 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m14:37:56.060185 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:37:56.061225 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m14:37:56.061225 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m14:37:56.061225 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m14:37:56.062219 [info ] [MainThread]: 
[0m14:37:56.062219 [info ] [MainThread]: Finished running 5 table models, 8 incremental models in 0 hours 0 minutes and 41.92 seconds (41.92s).
[0m14:37:56.066426 [debug] [MainThread]: Command end result
[0m14:37:56.105452 [info ] [MainThread]: 
[0m14:37:56.106424 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:37:56.107421 [info ] [MainThread]: 
[0m14:37:56.108419 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m14:37:56.109458 [debug] [MainThread]: Command `dbt run` succeeded at 14:37:56.109458 after 46.71 seconds
[0m14:37:56.109458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8DB0C4A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB90421FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB8FC7C770>]}
[0m14:37:56.110449 [debug] [MainThread]: Flushing usage events
[0m14:43:42.762493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B82B02C950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B82B7AB080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B82B7AB0E0>]}


============================== 14:43:42.762493 | b543f4e3-d06c-4f0c-84a2-218cb4e1f760 ==============================
[0m14:43:42.762493 [info ] [MainThread]: Running with dbt=1.8.9
[0m14:43:42.770493 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Moi\\Documents\\M2\\BI\\dbt\\dbtProjetBi\\alimentationDatawarehouse\\logs', 'profiles_dir': 'C:\\Users\\Moi\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --vars {execution_date: 2024-12-01}', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:43:44.400255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83C2DD820>]}
[0m14:43:44.480544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B82AF5F2C0>]}
[0m14:43:44.480544 [info ] [MainThread]: Registered adapter: bigquery=1.8.3
[0m14:43:45.463237 [debug] [MainThread]: checksum: 0a7db2ec7d53beccd4b5886de9d9aee72725a37baf7f8cf9a4ca0d4bb1123721, vars: {'execution_date': datetime.date(2024, 12, 1)}, profile: , target: , version: 1.8.9
[0m14:43:45.814781 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m14:43:45.814781 [debug] [MainThread]: previous checksum: 0a7db2ec7d53beccd4b5886de9d9aee72725a37baf7f8cf9a4ca0d4bb1123721, current checksum: e88dba9e2f09403cb99c6d862a8a24d4188893f218dc898d11a697b19a4fddc5
[0m14:43:45.814781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E289070>]}
[0m14:43:50.913380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E377FE0>]}
[0m14:43:51.147380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E356450>]}
[0m14:43:51.147380 [info ] [MainThread]: Found 17 models, 3 sources, 484 macros
[0m14:43:51.147380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83DFE82F0>]}
[0m14:43:51.155712 [info ] [MainThread]: 
[0m14:43:51.155712 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:43:51.163712 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen'
[0m14:43:51.163712 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:30.355413 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_projet-bi-isen_dataWarehouse'
[0m14:45:30.355413 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:30.754971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E3016D0>]}
[0m14:45:30.754971 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:45:30.754971 [info ] [MainThread]: 
[0m14:45:30.763306 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_StatutCommande
[0m14:45:30.763306 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.alimentationDatawarehouse.d_StatutCommande'
[0m14:45:30.763306 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_StatutCommande
[0m14:45:30.787803 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_StatutCommande"
[0m14:45:30.787803 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_StatutCommande
[0m14:45:30.796035 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_approvisionnement
[0m14:45:30.796035 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_StatutCommande, now model.alimentationDatawarehouse.f_approvisionnement)
[0m14:45:30.796035 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_approvisionnement
[0m14:45:30.804201 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_approvisionnement"
[0m14:45:30.804201 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_approvisionnement
[0m14:45:30.804201 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_commandeInternet
[0m14:45:30.804201 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_approvisionnement, now model.alimentationDatawarehouse.f_commandeInternet)
[0m14:45:30.812371 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_commandeInternet
[0m14:45:30.820701 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_commandeInternet"
[0m14:45:30.820701 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_commandeInternet
[0m14:45:30.820701 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_listeProduction
[0m14:45:30.820701 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_commandeInternet, now model.alimentationDatawarehouse.f_listeProduction)
[0m14:45:30.820701 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_listeProduction
[0m14:45:30.828932 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_listeProduction"
[0m14:45:30.829952 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_listeProduction
[0m14:45:30.829952 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:45:30.829952 [info ] [Thread-1 (]: 1 of 13 START sql table model dataWarehouse.stg_CommandeClient ................. [RUN]
[0m14:45:30.829952 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_listeProduction, now model.alimentationDatawarehouse.stg_CommandeClient)
[0m14:45:30.829952 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:45:30.838345 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m14:45:30.838345 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:45:30.871339 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:31.285121 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeClient"
[0m14:45:31.293356 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeClient"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        transactionID AS transaction_id,
        nomClient AS client_nom,
        prenomClient AS client_prenom,
        adresseClient AS client_adresse,
        contactClient AS client_contact,
        dateCommande AS date_commande,
        livreur AS livreur_nom,
        statut AS statut_commande,
        produit AS produit_nom,
        quantite AS produit_quantite,
        prixUnite AS produit_prix_unitaire
    FROM `projet-bi-isen`.`ODS`.`f_commandeInternet` WHERE ingestionTimestamp BETWEEN '2024-12-01' AND CURRENT_TIMESTAMP()      --mettre en variable pour la date de début
)

SELECT *
FROM raw_data
    );
  
[0m14:45:31.887595 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:2637c1de-d945-4db9-affc-9daebc30e051&page=queryresults
[0m14:45:34.658842 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83C83F650>]}
[0m14:45:34.658842 [info ] [Thread-1 (]: 1 of 13 OK created sql table model dataWarehouse.stg_CommandeClient ............ [[32mCREATE TABLE (263.0 rows, 28.8 KiB processed)[0m in 3.83s]
[0m14:45:34.658842 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeClient
[0m14:45:34.658842 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:45:34.667177 [info ] [Thread-1 (]: 2 of 13 START sql table model dataWarehouse.stg_CommandeFournisseur ............ [RUN]
[0m14:45:34.667177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeClient, now model.alimentationDatawarehouse.stg_CommandeFournisseur)
[0m14:45:34.667177 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:45:34.667177 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m14:45:34.667177 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:45:34.675426 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:35.018445 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_CommandeFournisseur"
[0m14:45:35.026785 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_CommandeFournisseur"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
      
    
    

    OPTIONS()
    as (
      WITH raw_data AS (
    SELECT
        ProvisionnementID AS provisionnement_id,
        FournisseurNom AS fournisseur_nom,
        FournisseurContact AS fournisseur_contact,
        FournisseurAdresse AS fournisseur_adresse,
        EntrepotNom AS entrepot_nom,
        EntrepotAdresse AS entrepot_adresse,
        EntrepotDescription AS entrepot_description,
        DateCommande AS date_commande,
        DateLivraison AS date_livraison,
        Materiaux AS materiaux_nom,
        Quantite AS quantite_provisionnee,
        Prix AS prix_provision,
        QualiteProvision AS qualite_provision
    FROM `projet-bi-isen`.`ODS`.`f_approvisionnement`
    WHERE ingestionTimestamp BETWEEN '2024-12-01' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m14:45:35.468337 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:7abcdebf-9456-4fea-8352-c9d3aa636512&page=queryresults
[0m14:45:37.949089 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E1BF2F0>]}
[0m14:45:37.949089 [info ] [Thread-1 (]: 2 of 13 OK created sql table model dataWarehouse.stg_CommandeFournisseur ....... [[32mCREATE TABLE (160.0 rows, 23.6 KiB processed)[0m in 3.28s]
[0m14:45:37.949089 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_CommandeFournisseur
[0m14:45:37.957368 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.stg_Production
[0m14:45:37.957368 [info ] [Thread-1 (]: 3 of 13 START sql table model dataWarehouse.stg_Production ..................... [RUN]
[0m14:45:37.957368 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_CommandeFournisseur, now model.alimentationDatawarehouse.stg_Production)
[0m14:45:37.957368 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.stg_Production
[0m14:45:37.965405 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.stg_Production"
[0m14:45:37.965405 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.stg_Production
[0m14:45:37.973402 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:38.498973 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.stg_Production"
[0m14:45:38.498973 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.stg_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.stg_Production"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`stg_Production`
      
    
    

    OPTIONS()
    as (
      -- models/staging/stg_Production.sql

WITH raw_data AS (
    SELECT
        ReleveProductionID AS production_id,
        Entrepot AS entrepot_nom,
        dateDebut AS production_debut,
        dateFin AS production_fin,
        produitProduit AS produit_nom,
        quantiteProduites AS quantite_produite,
        materiauxUtilise AS materiaux_utilises,
        quantiteUtilise AS quantite_utilisee
    FROM `projet-bi-isen`.`ODS`.`f_listeProduction`
    WHERE ingestionTimestamp BETWEEN '2024-12-01' AND CURRENT_TIMESTAMP()
)

SELECT *
FROM raw_data
    );
  
[0m14:45:39.174763 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:87f2050b-a4b0-4081-a8cc-60305be7bd66&page=queryresults
[0m14:45:41.502054 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E3A1340>]}
[0m14:45:41.502054 [info ] [Thread-1 (]: 3 of 13 OK created sql table model dataWarehouse.stg_Production ................ [[32mCREATE TABLE (175.0 rows, 15.4 KiB processed)[0m in 3.54s]
[0m14:45:41.502054 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.stg_Production
[0m14:45:41.502054 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Client
[0m14:45:41.502054 [info ] [Thread-1 (]: 4 of 13 START sql incremental model dataWarehouse.d_Client ..................... [RUN]
[0m14:45:41.510090 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.stg_Production, now model.alimentationDatawarehouse.d_Client)
[0m14:45:41.510090 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Client
[0m14:45:41.518324 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Client"
[0m14:45:41.526426 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Client
[0m14:45:41.584603 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:42.185612 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Client"
[0m14:45:42.185612 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Client: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Client"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Client` as DBT_INTERNAL_DEST
        using (-- a modifié

WITH base AS (
    SELECT
        transaction_id,
        client_nom AS Nom,
        client_prenom AS Prenom,
        client_adresse AS Adresse,
        client_contact AS Contact,
        ROW_NUMBER() OVER (
            PARTITION BY client_nom, client_prenom, client_adresse, client_contact 
            ORDER BY transaction_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS ClientID,
    Nom,
    Prenom,
    Adresse,
    Contact
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Client` AS target
        WHERE target.Nom = base.Nom
          AND target.Prenom = base.Prenom
          AND target.Adresse = base.Adresse
          AND target.Contact = base.Contact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)
    values
        (`ClientID`, `Nom`, `Prenom`, `Adresse`, `Contact`)


    
[0m14:45:42.719390 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:32c1a22a-7754-414f-9301-af90d394dfce&page=queryresults
[0m14:45:44.932158 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83B9E4FE0>]}
[0m14:45:44.932158 [info ] [Thread-1 (]: 4 of 13 OK created sql incremental model dataWarehouse.d_Client ................ [[32mMERGE (126.0 rows, 11.5 KiB processed)[0m in 3.42s]
[0m14:45:44.932158 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Client
[0m14:45:44.932158 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Livreur
[0m14:45:44.932158 [info ] [Thread-1 (]: 5 of 13 START sql incremental model dataWarehouse.d_Livreur .................... [RUN]
[0m14:45:44.940159 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Client, now model.alimentationDatawarehouse.d_Livreur)
[0m14:45:44.940159 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Livreur
[0m14:45:44.948165 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Livreur"
[0m14:45:44.956403 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Livreur
[0m14:45:44.964905 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:45.746251 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Livreur"
[0m14:45:45.746251 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Livreur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Livreur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Livreur` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT livreur_nom AS LivreurNom,
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS LivreurID,
    LivreurNom
FROM base


    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    WHERE NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Livreur` AS target
        WHERE target.LivreurNom = base.LivreurNom
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`LivreurID`, `LivreurNom`)
    values
        (`LivreurID`, `LivreurNom`)


    
[0m14:45:46.239347 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:5fb6bfdc-da7b-47a6-ac22-681042be87ce&page=queryresults
[0m14:45:48.755192 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E3960C0>]}
[0m14:45:48.755192 [info ] [Thread-1 (]: 5 of 13 OK created sql incremental model dataWarehouse.d_Livreur ............... [[32mMERGE (4.0 rows, 2.0 KiB processed)[0m in 3.82s]
[0m14:45:48.763882 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Livreur
[0m14:45:48.763882 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Entrepot
[0m14:45:48.763882 [info ] [Thread-1 (]: 6 of 13 START sql incremental model dataWarehouse.d_Entrepot ................... [RUN]
[0m14:45:48.763882 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Livreur, now model.alimentationDatawarehouse.d_Entrepot)
[0m14:45:48.771875 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Entrepot
[0m14:45:48.779890 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Entrepot"
[0m14:45:48.788862 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Entrepot
[0m14:45:48.796860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:50.989804 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Entrepot"
[0m14:45:50.989804 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Entrepot: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Entrepot"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        provisionnement_id,
        entrepot_nom AS EntrepotNom,
        entrepot_adresse AS EntrepotAdresse,
        entrepot_description AS EntrepotDescription,
        ROW_NUMBER() OVER (
            PARTITION BY entrepot_nom, entrepot_adresse 
            ORDER BY provisionnement_id
        ) AS rn -- Numérote chaque entrepôt pour éliminer les doublons
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

SELECT
    GENERATE_UUID() AS EntrepotID,
    EntrepotNom,
    EntrepotAdresse,
    EntrepotDescription
FROM base
WHERE rn = 1 -- Garde uniquement le premier entrepôt par groupe

    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` AS target
        WHERE target.EntrepotNom = base.EntrepotNom
          AND target.EntrepotAdresse = base.EntrepotAdresse
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)
    values
        (`EntrepotID`, `EntrepotNom`, `EntrepotAdresse`, `EntrepotDescription`)


    
[0m14:45:51.745339 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:357f6c03-c345-41e9-a690-5f8dd9755423&page=queryresults
[0m14:45:54.539593 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E61C5C0>]}
[0m14:45:54.539593 [info ] [Thread-1 (]: 6 of 13 OK created sql incremental model dataWarehouse.d_Entrepot .............. [[32mMERGE (1.0 rows, 9.8 KiB processed)[0m in 5.78s]
[0m14:45:54.547833 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Entrepot
[0m14:45:54.547833 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:45:54.547833 [info ] [Thread-1 (]: 7 of 13 START sql incremental model dataWarehouse.d_FournisseurDetails ......... [RUN]
[0m14:45:54.547833 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Entrepot, now model.alimentationDatawarehouse.d_FournisseurDetails)
[0m14:45:54.547833 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:45:54.569145 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m14:45:54.569145 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:45:54.591318 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:54.932802 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_FournisseurDetails"
[0m14:45:54.934872 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_FournisseurDetails: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_FournisseurDetails"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` as DBT_INTERNAL_DEST
        using (WITH base AS (
    SELECT
        DISTINCT provisionnement_id,
        fournisseur_nom AS FournisseurNom,
        fournisseur_contact AS FournisseurContact,
        fournisseur_adresse AS FournisseurAdresse,
        ROW_NUMBER() OVER (
            PARTITION BY fournisseur_nom, fournisseur_contact, fournisseur_adresse 
            ORDER BY provisionnement_id
        ) AS rn
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
)

-- Si le modèle est exécuté en mode 'incremental', dbt ajoute seulement les données non présentes dans la table
SELECT
    GENERATE_UUID() AS FournisseurID,
    FournisseurNom,
    FournisseurContact,
    FournisseurAdresse
FROM base
WHERE rn = 1

    -- Filtrage des nouveaux clients qui ne sont pas déjà dans d_Client
    AND NOT EXISTS (
        SELECT 1
        FROM `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` AS target
        WHERE target.FournisseurNom = base.FournisseurNom
          AND target.FournisseurContact = base.FournisseurContact
          AND target.FournisseurContact = base.FournisseurContact
    )

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)
    values
        (`FournisseurID`, `FournisseurNom`, `FournisseurContact`, `FournisseurAdresse`)


    
[0m14:45:55.801473 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:2a8b8f40-97ff-40ab-b282-bae710503e07&page=queryresults
[0m14:45:57.936656 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E9611C0>]}
[0m14:45:57.938697 [info ] [Thread-1 (]: 7 of 13 OK created sql incremental model dataWarehouse.d_FournisseurDetails .... [[32mMERGE (5.0 rows, 6.3 KiB processed)[0m in 3.39s]
[0m14:45:57.938697 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_FournisseurDetails
[0m14:45:57.938697 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Materiaux
[0m14:45:57.938697 [info ] [Thread-1 (]: 8 of 13 START sql table model dataWarehouse.d_Materiaux ........................ [RUN]
[0m14:45:57.949092 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_FournisseurDetails, now model.alimentationDatawarehouse.d_Materiaux)
[0m14:45:57.949092 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Materiaux
[0m14:45:57.959047 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Materiaux"
[0m14:45:57.969073 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Materiaux
[0m14:45:57.969073 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:59.258321 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Materiaux"
[0m14:45:59.266323 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Materiaux: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Materiaux"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Materiaux`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        MateriauxID,
        PrixUnitaire,
        Quantite, 
    FROM `projet-bi-isen.dataWarehouse.f_CommandeFournisseur` 
),
aggregated_data AS (
    SELECT
        MateriauxID,
        SUM(PrixUnitaire * Quantite) AS somme_ponderee_prix,
        SUM(Quantite) AS somme_quantite
    FROM filtered_data
    GROUP BY MateriauxID
),
ste AS(
    SELECT
    MateriauxID,
    CASE
        WHEN somme_quantite > 0 THEN ROUND(somme_ponderee_prix / somme_quantite,2)
        ELSE NULL
    END AS ValeurEstimee
    FROM aggregated_data
),
existing_data AS (
    SELECT 
        mat.MateriauxID,
        mat.MateriauxNom,
        s.ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat  -- Table actuelle d_Materiaux
    LEFT JOIN ste s ON mat.MateriauxID = s.MateriauxID
),
newDataValeruEstimee AS (
    SELECT
        materiaux_nom,
        ROUND(SUM(quantite_provisionnee*prix_provision)/SUM(quantite_provisionnee),2) AS ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
    GROUP BY materiaux_nom
),
new_data AS (
    SELECT
        GENERATE_UUID() AS MateriauxID,
        comf.materiaux_nom AS MateriauxNom,
        nd.ValeurEstimee AS ValeurEstimee
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur` comf
    LEFT JOIN newDataValeruEstimee nd ON comf.materiaux_nom = nd.materiaux_nom
    GROUP BY comf.materiaux_nom, nd.ValeurEstimee
),
unioned_data AS (
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM existing_data
    UNION ALL
    SELECT
        MateriauxID,
        MateriauxNom,
        ValeurEstimee
    FROM new_data
    WHERE MateriauxNom NOT IN (SELECT MateriauxNom FROM existing_data)
)

SELECT 
    MateriauxID,
    MateriauxNom,
    ValeurEstimee
FROM unioned_data
    );
  
[0m14:45:59.857600 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4ca88ba6-0fdb-497b-a5ac-43d73863ec02&page=queryresults
[0m14:46:03.214796 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E960C80>]}
[0m14:46:03.223068 [info ] [Thread-1 (]: 8 of 13 OK created sql table model dataWarehouse.d_Materiaux ................... [[32mCREATE TABLE (29.0 rows, 4.1 KiB processed)[0m in 5.27s]
[0m14:46:03.223068 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Materiaux
[0m14:46:03.223068 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Date
[0m14:46:03.231081 [info ] [Thread-1 (]: 9 of 13 START sql incremental model dataWarehouse.d_Date ....................... [RUN]
[0m14:46:03.231081 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Materiaux, now model.alimentationDatawarehouse.d_Date)
[0m14:46:03.231081 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Date
[0m14:46:03.248703 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Date"
[0m14:46:03.248703 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Date
[0m14:46:03.265285 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:46:03.814907 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Date"
[0m14:46:03.814907 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Date: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Date"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`d_Date` as DBT_INTERNAL_DEST
        using (WITH date_Client AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),
date_CommandeFournisseur AS (
    SELECT DISTINCT
        date_commande AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
    UNION ALL
    SELECT DISTINCT
        date_livraison AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),
date_Production AS (
    SELECT DISTINCT
        production_debut AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
    UNION ALL
    SELECT DISTINCT
        production_fin AS TriDate
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),
mixDate AS (
    SELECT DISTINCT
        TriDate
    FROM date_Client
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_CommandeFournisseur
    UNION ALL
    SELECT DISTINCT
        TriDate
    FROM date_Production
),
triDoublon AS (
    SELECT DISTINCT
        TriDate
    FROM mixDate
)

SELECT
    GENERATE_UUID() AS DateID,          -- Génération d'un identifiant unique pour chaque date
    EXTRACT(YEAR FROM TriDate) AS Annee,
    EXTRACT(MONTH FROM TriDate) AS Mois,
    EXTRACT(DAY FROM TriDate) AS Jours,
    TriDate AS DateViz
FROM triDoublon


-- Filtrage des nouvelles dates qui ne sont pas déjà présentes dans la table cible
WHERE NOT EXISTS (
    SELECT 1
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Date` AS target
    WHERE 
        target.Annee = EXTRACT(YEAR FROM TriDate) AND
        target.Mois = EXTRACT(MONTH FROM TriDate) AND
        target.Jours = EXTRACT(DAY FROM TriDate)
)

        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`DateID`, `Jours`, `Mois`, `Annee`, `DateViz`)
    values
        (`DateID`, `Jours`, `Mois`, `Annee`, `DateViz`)


    
[0m14:46:04.180418 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:9403e77b-64fc-4901-b510-03518b27c68c&page=queryresults
[0m14:46:06.739115 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E608380>]}
[0m14:46:06.739115 [info ] [Thread-1 (]: 9 of 13 OK created sql incremental model dataWarehouse.d_Date .................. [[32mMERGE (42.0 rows, 7.3 KiB processed)[0m in 3.51s]
[0m14:46:06.747986 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Date
[0m14:46:06.747986 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.d_Produit
[0m14:46:06.747986 [info ] [Thread-1 (]: 10 of 13 START sql table model dataWarehouse.d_Produit ......................... [RUN]
[0m14:46:06.747986 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Date, now model.alimentationDatawarehouse.d_Produit)
[0m14:46:06.747986 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.d_Produit
[0m14:46:06.763989 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.d_Produit"
[0m14:46:06.763989 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.d_Produit
[0m14:46:06.772353 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:46:07.097755 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.d_Produit"
[0m14:46:07.097755 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.d_Produit: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.d_Produit"} */

  
    

    create or replace table `projet-bi-isen`.`dataWarehouse`.`d_Produit`
      
    
    

    OPTIONS()
    as (
      

WITH filtered_data AS (
    SELECT
        prod.ProduitID,
        prod.QuantiteProduite,
        prod.MateriauxID,
        mat.ValeurEstimee AS PrixUnitaire,
        prod.QuantiteUtilise 
    FROM `projet-bi-isen.dataWarehouse.f_Production` prod
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat ON prod.MateriauxID = mat.MateriauxID
),
aggregated_data AS (
    SELECT
        ProduitID,
        SUM(QuantiteProduite) AS somme_quantite_produite,
        SUM(PrixUnitaire * QuantiteUtilise) AS somme_ponderee_prix,
        SUM(QuantiteUtilise) AS somme_quantite
    FROM filtered_data
    GROUP BY ProduitID
),
ste AS (
    SELECT
        ProduitID,
        CASE
            WHEN somme_quantite > 0 THEN ROUND((somme_ponderee_prix / somme_quantite) / somme_quantite_produite, 2)
            ELSE NULL
        END AS CoutdeRevient
    FROM aggregated_data
),
existing_data AS (
    SELECT 
        prod.ProduitID,
        prod.ProduitNom,
        s.CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`d_Produit` prod  -- Table actuelle d_Produit
    LEFT JOIN ste s ON prod.ProduitID = s.ProduitID
),
newdataCoutDeRevient AS (
    SELECT
        prod.produit_nom,
        ROUND(SUM(mat.ValeurEstimee*prod.quantite_utilisee)/prod.quantite_produite,2) AS CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production` prod
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` mat ON prod.materiaux_utilises = mat.MateriauxNom
    GROUP BY produit_nom, prod.quantite_produite
),
new_data AS (
    SELECT
        GENERATE_UUID() AS ProduitID,
        prod.produit_nom AS ProduitNom,
        ncdt.CoutdeRevient AS CoutdeRevient
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production` prod
    LEFT JOIN newdataCoutDeRevient ncdt ON prod.produit_nom = ncdt.produit_nom
    GROUP BY prod.produit_nom, ncdt.CoutdeRevient
),
unioned_data AS (
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM existing_data
    UNION ALL
    SELECT
        ProduitID,
        ProduitNom,
        CoutdeRevient
    FROM new_data
    -- Amélioration de la condition pour éviter les NULL avec NOT IN
    WHERE ProduitNom NOT IN (SELECT ProduitNom FROM existing_data WHERE ProduitNom IS NOT NULL)
)

SELECT 
    ProduitID,
    ProduitNom,
    CoutdeRevient
FROM unioned_data
    );
  
[0m14:46:07.602193 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:b31c5223-9283-4185-8ace-1fd47280514c&page=queryresults
[0m14:46:10.765640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E98E720>]}
[0m14:46:10.765640 [info ] [Thread-1 (]: 10 of 13 OK created sql table model dataWarehouse.d_Produit .................... [[32mCREATE TABLE (30.0 rows, 9.3 KiB processed)[0m in 4.02s]
[0m14:46:10.773952 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.d_Produit
[0m14:46:10.773952 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:46:10.773952 [info ] [Thread-1 (]: 11 of 13 START sql incremental model dataWarehouse.f_CommandeFournisseur ....... [RUN]
[0m14:46:10.773952 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.d_Produit, now model.alimentationDatawarehouse.f_CommandeFournisseur)
[0m14:46:10.773952 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:46:10.782239 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m14:46:10.790247 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:46:10.808126 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:46:11.293559 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeFournisseur"
[0m14:46:11.293559 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeFournisseur: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeFournisseur"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeFournisseur` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        provisionnement_id AS ProvisionnementID,
        fournisseur_nom,
        fournisseur_contact,
        fournisseur_adresse,
        entrepot_nom,
        entrepot_adresse,
        entrepot_description,
        date_commande AS DateCommandeProvision,
        date_livraison AS DateLivraisonProvision,
        materiaux_nom,
        quantite_provisionnee AS Quantite,
        prix_provision AS PrixUnitaire,
        qualite_provision AS QualiteProvision
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeFournisseur`
),

mapped_data AS (
    SELECT
        sd.ProvisionnementID,
        dim_materiaux.MateriauxID,
        dim_fournisseur.FournisseurID,
        dim_entrepot.EntrepotID,
        dim_date.DateID AS DateCommandeProvision,
        dim_dateLivraison.DateID AS DateLivraisonProvision,
        sd.Quantite,
        sd.PrixUnitaire,
        sd.QualiteProvision
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_FournisseurDetails` dim_fournisseur
        ON dim_fournisseur.FournisseurNom = sd.fournisseur_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommandeProvision)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommandeProvision)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommandeProvision)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_dateLivraison
        ON dim_dateLivraison.Annee = EXTRACT(YEAR FROM sd.DateLivraisonProvision)
        AND dim_dateLivraison.Mois = EXTRACT(MONTH FROM sd.DateLivraisonProvision)
        AND dim_dateLivraison.Jours = EXTRACT(DAY FROM sd.DateLivraisonProvision)
)

SELECT
    ProvisionnementID,
    MateriauxID,
    FournisseurID,
    EntrepotID,
    DateCommandeProvision,
    DateLivraisonProvision,
    Quantite,
    PrixUnitaire,
    QualiteProvision
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `DateLivraisonProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)
    values
        (`ProvisionnementID`, `MateriauxID`, `FournisseurID`, `EntrepotID`, `DateCommandeProvision`, `DateLivraisonProvision`, `Quantite`, `PrixUnitaire`, `QualiteProvision`)


    
[0m14:46:11.788396 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:4a0797c5-3c88-40bf-9cc8-03052f2c174c&page=queryresults
[0m14:46:13.764649 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E99F3E0>]}
[0m14:46:13.772825 [info ] [Thread-1 (]: 11 of 13 OK created sql incremental model dataWarehouse.f_CommandeFournisseur .. [[32mMERGE (160.0 rows, 17.2 KiB processed)[0m in 2.99s]
[0m14:46:13.772825 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeFournisseur
[0m14:46:13.772825 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_CommandeClient
[0m14:46:13.772825 [info ] [Thread-1 (]: 12 of 13 START sql incremental model dataWarehouse.f_CommandeClient ............ [RUN]
[0m14:46:13.780831 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeFournisseur, now model.alimentationDatawarehouse.f_CommandeClient)
[0m14:46:13.780831 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_CommandeClient
[0m14:46:13.830673 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m14:46:13.830673 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_CommandeClient
[0m14:46:13.838926 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:46:14.206724 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_CommandeClient"
[0m14:46:14.206724 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_CommandeClient: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_CommandeClient"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_CommandeClient` as DBT_INTERNAL_DEST
        using (WITH  __dbt__cte__d_StatutCommande as (


SELECT *
FROM `projet-bi-isen.dataWarehouse.d_StatutCommande`
), staged_data AS (
    SELECT
        client_nom,
        client_prenom,
        client_adresse,
        client_contact,
        produit_nom,
        livreur_nom,
        statut_commande,
        produit_prix_unitaire AS PrixProduitVente,
        produit_quantite AS Quantite,
        date_commande AS DateCommande
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_CommandeClient`
),

mapped_data AS (
    SELECT
        GENERATE_UUID() AS CommandeID,
        -- Recherche des clés dans les tables de dimension
        dim_client.ClientID,
        dim_product.ProduitID,
        dim_livreur.LivreurID,
        dim_statut.StatutID,
        sd.PrixProduitVente,
        sd.Quantite,
        dim_date.DateID AS DateCommande
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Client` dim_client
        ON dim_client.Nom = sd.client_nom
        AND dim_client.Prenom = sd.client_prenom
        AND dim_client.Adresse = sd.client_adresse
        AND dim_client.Contact = sd.client_contact
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_product
        ON dim_product.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Livreur` dim_livreur
        ON dim_livreur.LivreurNom = sd.livreur_nom
    LEFT JOIN __dbt__cte__d_StatutCommande dim_statut
        ON dim_statut.Statut = sd.statut_commande
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date
        ON dim_date.Annee = EXTRACT(YEAR FROM sd.DateCommande)
        AND dim_date.Mois = EXTRACT(MONTH FROM sd.DateCommande)
        AND dim_date.Jours = EXTRACT(DAY FROM sd.DateCommande)
)

SELECT
    CommandeID,
    ClientID,
    ProduitID,
    LivreurID,
    StatutID,
    PrixProduitVente,
    Quantite,
    DateCommande
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)
    values
        (`CommandeID`, `ClientID`, `ProduitID`, `LivreurID`, `StatutID`, `PrixProduitVente`, `Quantite`, `DateCommande`)


    
[0m14:46:15.198232 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:ccbc0e4b-b4e3-44a2-a429-70af16994903&page=queryresults
[0m14:46:17.698935 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83C85E8A0>]}
[0m14:46:17.707025 [info ] [Thread-1 (]: 12 of 13 OK created sql incremental model dataWarehouse.f_CommandeClient ....... [[32mMERGE (263.0 rows, 36.9 KiB processed)[0m in 3.93s]
[0m14:46:17.707344 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_CommandeClient
[0m14:46:17.707344 [debug] [Thread-1 (]: Began running node model.alimentationDatawarehouse.f_Production
[0m14:46:17.707344 [info ] [Thread-1 (]: 13 of 13 START sql incremental model dataWarehouse.f_Production ................ [RUN]
[0m14:46:17.715597 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.alimentationDatawarehouse.f_CommandeClient, now model.alimentationDatawarehouse.f_Production)
[0m14:46:17.715597 [debug] [Thread-1 (]: Began compiling node model.alimentationDatawarehouse.f_Production
[0m14:46:17.732434 [debug] [Thread-1 (]: Writing injected SQL for node "model.alimentationDatawarehouse.f_Production"
[0m14:46:17.732434 [debug] [Thread-1 (]: Began executing node model.alimentationDatawarehouse.f_Production
[0m14:46:17.740442 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:46:18.033755 [debug] [Thread-1 (]: Writing runtime sql for node "model.alimentationDatawarehouse.f_Production"
[0m14:46:18.033755 [debug] [Thread-1 (]: On model.alimentationDatawarehouse.f_Production: /* {"app": "dbt", "dbt_version": "1.8.9", "profile_name": "alimentationDatawarehouse", "target_name": "dev", "node_id": "model.alimentationDatawarehouse.f_Production"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `projet-bi-isen`.`dataWarehouse`.`f_Production` as DBT_INTERNAL_DEST
        using (WITH staged_data AS (
    SELECT
        production_id AS ProductionID,
        entrepot_nom,
        production_debut AS DateDebut,
        production_fin AS DateFin,
        produit_nom,
        quantite_produite AS QuantiteProduite,
        materiaux_utilises,
        quantite_utilisee AS QuantiteUtilise
    FROM `projet-bi-isen`.`dataWarehouse`.`stg_Production`
),

mapped_data AS (
    SELECT
        sd.ProductionID,
        dim_produit.ProduitID,
        sd.QuantiteProduite,
        dim_materiaux.MateriauxID,
        sd.QuantiteUtilise,
        dim_entrepot.EntrepotID,
        dim_date_debut.DateID AS DateDebutID,
        dim_date_fin.DateID AS DateFinID
    FROM staged_data sd
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Produit` dim_produit
        ON dim_produit.ProduitNom = sd.produit_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Materiaux` dim_materiaux
        ON dim_materiaux.MateriauxNom = sd.materiaux_utilises
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Entrepot` dim_entrepot
        ON dim_entrepot.EntrepotNom = sd.entrepot_nom
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_debut
        ON dim_date_debut.Annee = EXTRACT(YEAR FROM sd.DateDebut)
        AND dim_date_debut.Mois = EXTRACT(MONTH FROM sd.DateDebut)
        AND dim_date_debut.Jours = EXTRACT(DAY FROM sd.DateDebut)
    LEFT JOIN `projet-bi-isen`.`dataWarehouse`.`d_Date` dim_date_fin
        ON dim_date_fin.Annee = EXTRACT(YEAR FROM sd.DateFin)
        AND dim_date_fin.Mois = EXTRACT(MONTH FROM sd.DateFin)
        AND dim_date_fin.Jours = EXTRACT(DAY FROM sd.DateFin)
)

SELECT
    ProductionID,
    ProduitID,
    QuantiteProduite,
    MateriauxID,
    QuantiteUtilise,
    EntrepotID,
    DateDebutID AS DateDebut,
    DateFinID AS DateFin
FROM mapped_data
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)
    values
        (`ProductionID`, `ProduitID`, `QuantiteProduite`, `MateriauxID`, `QuantiteUtilise`, `EntrepotID`, `DateDebut`, `DateFin`)


    
[0m14:46:18.712262 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=projet-bi-isen&j=bq:EU:dbf2c439-2059-430d-9dff-ce06dc5470f9&page=queryresults
[0m14:46:20.914930 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b543f4e3-d06c-4f0c-84a2-218cb4e1f760', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B827D76C90>]}
[0m14:46:20.914930 [info ] [Thread-1 (]: 13 of 13 OK created sql incremental model dataWarehouse.f_Production ........... [[32mMERGE (175.0 rows, 19.6 KiB processed)[0m in 3.21s]
[0m14:46:20.914930 [debug] [Thread-1 (]: Finished running node model.alimentationDatawarehouse.f_Production
[0m14:46:20.923128 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:46:20.923128 [debug] [MainThread]: Connection 'list_projet-bi-isen' was properly closed.
[0m14:46:20.925284 [debug] [MainThread]: Connection 'list_projet-bi-isen_dataWarehouse' was properly closed.
[0m14:46:20.925284 [debug] [MainThread]: Connection 'model.alimentationDatawarehouse.f_Production' was properly closed.
[0m14:46:20.925284 [info ] [MainThread]: 
[0m14:46:20.925284 [info ] [MainThread]: Finished running 5 table models, 8 incremental models in 0 hours 2 minutes and 29.77 seconds (149.77s).
[0m14:46:20.933203 [debug] [MainThread]: Command end result
[0m14:46:21.023678 [info ] [MainThread]: 
[0m14:46:21.023678 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:46:21.023678 [info ] [MainThread]: 
[0m14:46:21.031933 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m14:46:21.031933 [debug] [MainThread]: Command `dbt run` succeeded at 14:46:21.031933 after 158.44 seconds
[0m14:46:21.031933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E34D160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B83E1D0F20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B82B6BCCE0>]}
[0m14:46:21.031933 [debug] [MainThread]: Flushing usage events
